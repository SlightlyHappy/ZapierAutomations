{"metadata": {"version": "gdpr_v1"}, "zaps": [{"id": 268045568, "title": "Working Linear Simple", "status": "off", "nodes": {"268045568": {"id": 268045568, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "read", "params": {"label_ids": "INBOX"}, "meta": {"$editor": {"$zap_guesser": {"generated_by_ai": false, "zap_guesser_execution_id": "0192e9c9-d0a7-7e64-adf1-7f3c50f4c265", "$ai_field_suggestions": {"choices": {"label_ids": ["CHAT"]}}}, "has_automatic_issues": false}, "parammap": {"label_ids": "INBOX"}, "stepTitle": "New Inbound Email"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": null, "root_id": null, "action": "message", "selected_api": "GoogleMailV2CLIAPI@1.1.11", "title": "Working Linear Simple", "authentication_id": 50294643, "created_at": "2024-11-20T10:16:09+00:00", "last_changed": "2024-11-28T10:46:33+00:00"}, "268045569": {"id": 268045569, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"storage_source": "one_drive", "folder_id": "01HP6PBRQ3HBO6QF7FOVCKMDW3YKCHI4FK", "workbook_id": "01HP6PBRVTSAHB5DDIORGIQNZYWYFVMKU7", "worksheet_id": "{00000000-0001-0000-0000-000000000000}", "COL$A": "{{268045568__body_plain}}", "COL$B": "", "COL$C": ""}, "meta": {"$editor": {"has_automatic_issues": false, "$zap_guesser": {"$ai_field_suggestions": {"fields": {"COL$A": ["268045568__body_plain"], "COL$C": ["268045568__from__email", "268045568__body_plain"]}}}}, "parammap": {"folder_id": "Zapier Automation", "workbook_id": "DataBase Zap Automation.xlsx", "worksheet_id": "Links Raw Data"}, "stepTitle": "Add Row"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 268045568, "root_id": 268045568, "action": "add_row", "selected_api": "ExcelCLIAPI@1.4.2", "title": null, "authentication_id": 50800439, "created_at": "2024-11-20T10:16:09+00:00", "last_changed": "2024-11-26T21:05:29+00:00"}, "268045570": {"id": 268045570, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"code": "import requests\nfrom requests.adapters import HTTPAdapter\nfrom urllib.parse import urlparse, urlunparse\nfrom bs4 import BeautifulSoup\n\n# Check urllib3 version to set the correct parameter\ntry:\n    import urllib3\n    urllib3_version = urllib3.__version__\nexcept ImportError:\n    # Default to an older version if urllib3 is not available\n    urllib3_version = '1.25.0'\n\n# Define the Retry class with the appropriate argument\ndef get_retry():\n    from urllib3.util import Retry\n    if urllib3_version >= '1.26.0':\n        return Retry(\n            total=3,\n            backoff_factor=0.3,\n            status_forcelist=[500, 502, 503, 504],\n            allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\", \"POST\"]\n        )\n    else:\n        return Retry(\n            total=3,\n            backoff_factor=0.3,\n            status_forcelist=[500, 502, 503, 504],\n            method_whitelist=[\"HEAD\", \"GET\", \"OPTIONS\", \"POST\"]\n        )\n\n# Get the URL from the input data\nurl = input_data.get('url', '').strip()\n\n# Clean the URL to remove any unwanted characters like '\\r' and '\\n'\nparsed_url = urlparse(url)\nclean_path = parsed_url.path.replace('\\r', '').replace('\\n', '')\nclean_url = urlunparse(parsed_url._replace(path=clean_path))\n\nheaders = {\n    'User-Agent': (\n        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n        '(KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n    )\n}\n\n# Set up a session with retry strategy\nsession = requests.Session()\n\nretry_strategy = get_retry()\nadapter = HTTPAdapter(max_retries=retry_strategy)\nsession.mount('http://', adapter)\nsession.mount('https://', adapter)\n\ntry:\n    # Fetch the HTML content with a timeout\n    response = session.get(clean_url, headers=headers, timeout=5)\n    response.raise_for_status()  # Raise an exception for bad status codes\n\n    # Parse the HTML using BeautifulSoup\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    # Extract the entire content of the <body> tag, including all HTML\n    body_tag = soup.body\n    if body_tag:\n        body_html = str(body_tag)\n    else:\n        body_html = ''\n\n    # Output the body HTML\n    output = {'html': body_html.strip()}\n\nexcept requests.exceptions.RequestException as e:\n    output = {'error': f'Error fetching URL: {e}'}\n\n# For demonstration purposes, print the output\nprint(output)", "input": {"url": "{{268045568__body_plain}}"}}, "meta": {"$editor": {"has_automatic_issues": false, "$zap_guesser": {"generated_by_ai": true}}, "parammap": {}, "stepTitle": "Grab HTML"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 268045569, "root_id": 268045568, "action": "01929fad-d3f6-c57c-1b40-ef8691be1f86", "selected_api": "CodeCLIAPI@1.0.1", "title": "Grab HTML", "authentication_id": null, "created_at": "2024-11-20T10:16:09+00:00", "last_changed": "2024-11-26T21:05:29+00:00"}, "268045571": {"id": 268045571, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"code": "import re  # Using the standard 're' module\nimport json\n\n# Enhanced regex patterns compatible with 're' module\npatterns = {\n    'email': r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\",\n\n    'phone': r\"(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{1,4}\\)?[-.\\s]?)*\\d{3,4}\",\n\n    'fax': r\"(?:Fax|F|\u4f20\u771f|\u30d5\u30a1\u30c3\u30af\u30b9|\ud329\uc2a4|\u092b\u0948\u0915\u094d\u0938|\u0442\u0435\u043b|\u0444\u0430\u043a\u0441)[.:]?\\s*(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{1,4}\\)?[-.\\s]?)*\\d{3,4}\",\n\n    'name': r\"\"\"\n        (?:(?:Dr|Mr|Mrs|Ms|Miss|Professor|Prof|MD|DVM|Docteur|V\u00e9t\u00e9rinaire|Asv|\n        \u0938\u0930\u094d\u0935|\u0936\u094d\u0930\u0940|\u0936\u094d\u0930\u0940\u092e\u0924\u0940|Herr|Frau|M\\.|Mme|Mlle|Se\u00f1or|Se\u00f1ora|Se\u00f1orita|\u5148\u751f|\u5973\u58eb|\u6559\u6388|\u535a\u58eb|\uc120\uc0dd\ub2d8|\uad50\uc218\ub2d8|\n        \u0414\u0440|\u0413-\u043d|\u0413-\u0436\u0430|\u0413\u043e\u0441\u043f\u043e\u0434\u0438\u043d|\u0413\u043e\u0441\u043f\u043e\u0436\u0430|\u041f\u0440\u043e\u0444)\\.?\\s+)?\n        (?:[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)+)\n    \"\"\",\n\n    'alt_name': r\"(?:[A-Z][a-z]+(?:\\s+[A-Z][a-z]+){1,2})\",\n\n    'address': r\"\"\"\n        \\d{1,5}\\s+[A-Za-z0-9\\s.,'-]+\n    \"\"\",\n\n    'address_fallback': r\"[A-Za-z0-9\\s.,'-]+\",\n\n    'social_links': {\n        'facebook': r\"(?:https?:\\/\\/)?(?:www\\.)?(?:facebook|fb)\\.(?:com|cn)\\/[^\\/\\s]+\\/?\",\n        'twitter': r\"(?:https?:\\/\\/)?(?:www\\.)?twitter\\.com\\/[^\\/\\s]+\\/?\",\n        'instagram': r\"(?:https?:\\/\\/)?(?:www\\.)?instagram\\.com\\/[^\\/\\s]+\\/?\",\n        'linkedin': r\"(?:https?:\\/\\/)?(?:www\\.)?linkedin\\.com\\/(?:company|in)\\/[^\\/\\s]+\\/?\",\n        'youtube': r\"(?:https?:\\/\\/)?(?:www\\.)?youtube\\.com\\/(?:channel|user)\\/[^\\/\\s]+\\/?\"\n    }\n}\n\n# Compile patterns with appropriate flags\ncompiled_patterns = {\n    'email': re.compile(patterns['email'], re.IGNORECASE),\n    'phone': re.compile(patterns['phone']),\n    'fax': re.compile(patterns['fax'], re.IGNORECASE),\n    'name': re.compile(patterns['name'], re.VERBOSE),\n    'alt_name': re.compile(patterns['alt_name']),\n    'address': re.compile(patterns['address'], re.VERBOSE),\n    'address_fallback': re.compile(patterns['address_fallback']),\n    'social_links': {\n        platform: re.compile(pattern, re.IGNORECASE)\n        for platform, pattern in patterns['social_links'].items()\n    }\n}\n\ndef clean_phone(phone):\n    \"\"\"Clean phone numbers without phonenumbers library\"\"\"\n    try:\n        # Remove all non-digit characters\n        digits = re.sub(r\"\\D\", \"\", phone)\n\n        # Return digits if length is acceptable\n        if len(digits) >= 7:\n            return digits\n        else:\n            return None  # Discard short numbers\n    except Exception:\n        return None\n\ndef is_likely_name(name):\n    \"\"\"Filter out false positives in name detection\"\"\"\n    common_words = {'Home', 'Page', 'Menu', 'Contact', 'About', 'Services', 'Privacy', 'Policy',\n                    'Terms', 'Conditions', 'Search', 'Main', 'Content', 'Navigation', 'Careers',\n                    'Emergency', 'FAQ', 'Log', 'In', 'Out', 'Find', 'More'}\n    words = name.split()\n    return (\n        len(words) >= 2 and\n        not any(word in common_words for word in words) and\n        not any(char.isdigit() for char in name) and\n        all(word[0].isupper() for word in words if word)\n    )\n\ndef extract_info(html):\n    \"\"\"Extract all contact information from HTML\"\"\"\n    try:\n        # Remove style and script contents\n        html_no_style = re.sub(r'<style[^>]*>[\\s\\S]*?</style>', '', html, flags=re.IGNORECASE)\n        html_no_script = re.sub(r'<script[^>]*>[\\s\\S]*?</script>', '', html_no_style, flags=re.IGNORECASE)\n\n        # Convert HTML to text\n        text_content = re.sub(r'<[^>]+>', ' ', html_no_script, flags=re.IGNORECASE)\n        text_content = re.sub(r'\\s+', ' ', text_content, flags=re.IGNORECASE).strip()\n\n        # Extract emails\n        emails = list(set(compiled_patterns['email'].findall(text_content)))\n\n        # Extract and clean phone numbers\n        phones = []\n        phone_matches = compiled_patterns['phone'].findall(text_content)\n        for phone in phone_matches:\n            cleaned = clean_phone(phone)\n            if cleaned and cleaned not in phones:\n                phones.append(cleaned)\n\n        # Extract and clean fax numbers\n        fax_numbers = []\n        fax_contexts = compiled_patterns['fax'].finditer(text_content)\n        for match in fax_contexts:\n            fax = clean_phone(match.group())\n            if fax and fax not in fax_numbers:\n                fax_numbers.append(fax)\n\n        # Extract names\n        names = set()\n        titled_names = compiled_patterns['name'].findall(text_content)\n        names.update(name.strip() for name in titled_names if is_likely_name(name.strip()))\n\n        potential_names = compiled_patterns['alt_name'].findall(text_content)\n        names.update(name.strip() for name in potential_names if is_likely_name(name.strip()))\n\n        # Extract addresses using primary pattern\n        addresses = list(set(compiled_patterns['address'].findall(text_content)))\n\n        # Filter out irrelevant addresses\n        addresses = [addr.strip() for addr in addresses if len(addr.strip().split()) > 2]\n\n        # Extract all href attributes\n        href_pattern = re.compile(r'href=[\\'\"]?([^\\'\" >]+)', re.IGNORECASE)\n        hrefs = href_pattern.findall(html)\n\n        # Extract social media links\n        social_links = {}\n        for platform, pattern in compiled_patterns['social_links'].items():\n            matches = [href for href in hrefs if pattern.search(href)]\n            if matches:\n                social_links[platform] = list(set(matches))\n\n        return {\n            'emails': emails,\n            'phones': phones,\n            'fax': fax_numbers,\n            'names': list(names),\n            'addresses': addresses,\n            'social_links': social_links\n        }\n\n    except Exception as e:\n        print(f\"Error in extraction: {str(e)}\")\n        return {\n            'emails': [],\n            'phones': [],\n            'fax': [],\n            'names': [],\n            'addresses': [],\n            'social_links': {}\n        }\n\n# Get HTML content from input_data provided by Zapier\nhtml_content = input_data.get('html', '')\n\n# Process HTML and prepare output\ntry:\n    if not html_content:\n        output = {\n            'success': False,\n            'error': 'No HTML content provided',\n            'data': None\n        }\n    else:\n        result = extract_info(html_content)\n        output = {\n            'success': True,\n            'error': None,\n            'data': result\n        }\n\nexcept Exception as e:\n    output = {\n        'success': False,\n        'error': str(e),\n        'data': None\n    }\n\n# Print the final output with required ID\nprint(json.dumps({\n    'output': output,\n    'id': '5fgunBIH5nfcaPgwiymwcjuT28AwOIJs'\n}))", "input": {"html": "{{268045570__html}}"}}, "meta": {"$editor": {"has_automatic_issues": false, "$zap_guesser": {"generated_by_ai": true}}, "parammap": {}, "stepTitle": "Parse the HTML"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 268045570, "root_id": 268045568, "action": "01929fad-d3f6-c57c-1b40-ef8691be1f86", "selected_api": "CodeCLIAPI@1.0.1", "title": "BS4 the HTML", "authentication_id": null, "created_at": "2024-11-20T10:16:09+00:00", "last_changed": "2024-11-26T21:05:29+00:00"}, "268045572": {"id": 268045572, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"storage_source": "one_drive", "folder_id": "01HP6PBRQ3HBO6QF7FOVCKMDW3YKCHI4FK", "workbook_id": "01HP6PBRVTSAHB5DDIORGIQNZYWYFVMKU7", "worksheet_id": "{128204DB-4ECC-484D-8165-72F5C76A50C8}", "COL$A": ["{{268045571__data__emails}}"], "COL$B": ["{{268045571__data__phones}}"], "COL$C": ["{{268045571__data__fax}}"], "COL$D": ["{{268045571__data__names}}"], "COL$E": ["{{268045571__data__addresses}}"], "COL$F": ["{{268045571__data__social_links__facebook}}"]}, "meta": {"$editor": {"has_automatic_issues": false, "$zap_guesser": {"$ai_field_suggestions": {"fields": {"COL$A": ["268045568__body_plain"], "COL$B": ["268045571__data__phones"], "COL$C": ["268045571__data__fax"], "COL$D": ["268045571__data__names"], "COL$E": ["268045568__body_plain", "268045571__data__addresses"], "COL$F": ["268045571__data__social_links__facebook", "268045571__data__social_links__facebook"]}}}}, "parammap": {"folder_id": "Zapier Automation", "workbook_id": "DataBase Zap Automation.xlsx", "worksheet_id": "Raw Data"}}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 268045571, "root_id": 268045568, "action": "add_rows", "selected_api": "ExcelCLIAPI@1.4.2", "title": null, "authentication_id": 50800439, "created_at": "2024-11-20T10:16:09+00:00", "last_changed": "2024-11-26T21:05:29+00:00"}, "268045573": {"id": 268045573, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"model": "gpt-3.5-turbo-instruct", "temperature": "0.7", "top_p": "1", "frequency_penalty": "0", "presence_penalty": "0", "prompt": "I have this parsed data from a website - \n{{268045571__data__names}}\n{{268045571__data__addresses}}\n{{268045571__data__social_links__instagram}}\n{{268045571__data__social_links__facebook}}\nI want you to make sense of this information - look up the information online, and have a write-up about the institution this information is related to. \nSee if you can also find names of people associated with this institution. \nThe output should be text. "}, "meta": {"$editor": {"has_automatic_issues": true, "$zap_guesser": {"$ai_field_suggestions": {"choices": {"model": ["gpt-3.5-turbo-instruct"]}}}}, "parammap": {"model": "gpt-3.5-turbo-instruct"}, "stepTitle": "Find General Information"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 268045572, "root_id": 268045568, "action": "send_prompt", "selected_api": "OpenAiCLIAPI@1.19.2", "title": null, "authentication_id": 50832433, "created_at": "2024-11-20T10:16:09+00:00", "last_changed": "2024-11-26T21:05:29+00:00"}, "268045574": {"id": 268045574, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"model": "gpt-3.5-turbo-instruct", "temperature": "0.7", "top_p": "1", "frequency_penalty": "0", "presence_penalty": "0", "prompt": "I have this list of parsed information from a website - \n{{268045571__data__addresses}}\n\n{{268045571__data__social_links__facebook}}\n{{268045571__data__social_links__instagram}}\n{{268045571__data__names}}\nI want you to make sense of the information - use online searches. \nI want you to find the address and validate it using Google searches, or by navigating to the links above. \nThen output just the validated address. \nThere can be no other answers than the validated address. Use the internet. \nThe output should be just the address of the business in that locality's format"}, "meta": {"$editor": {"has_automatic_issues": true}, "parammap": {"model": "gpt-3.5-turbo-instruct"}, "stepTitle": "Address Validation by AI"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 268045573, "root_id": 268045568, "action": "send_prompt", "selected_api": "OpenAiCLIAPI@1.19.2", "title": null, "authentication_id": 50832433, "created_at": "2024-11-20T10:16:09+00:00", "last_changed": "2024-11-26T21:05:29+00:00"}, "268045575": {"id": 268045575, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"model": "gpt-3.5-turbo-instruct", "temperature": "0.7", "top_p": "1", "frequency_penalty": "0", "presence_penalty": "0", "prompt": "This is an extract from a parsed HTML (provided information) - it could include links, names, keywords, phone numbers, emails, social media accounts, etc. \n{{268045568__body_plain}}\n{{268045571__data__social_links__instagram}}\n{{268045571__data__social_links__facebook}}\n{{268045571__data__phones}}\n{{268045571__data__fax}}\n{{268045571__data__emails}}\n{{268045571__data__names}}\n{{268045571__data__addresses}}\n\nYour task is to extract, organize, and validate information using the internet based on the following categories:\n\t1.\tCompany name (EN)\n\t2.\tCompany name (Local Language if any)\n\t3.\tCompany Website\n\t4.\tCompany owner\n\t5.\tVET/CRO/BIO/MED/Academia\n\t6.\tCountry\n\t7.\tLF MRI / HF MRI / CT SCAN\n\t8.\tMRI Manufacturer\n\t9.\tMRI Type\n\t10.\tMRI name\n\t11.\tCivility\n\t12.\tContact first name\n\t13.\tContact last name\n\t14.\tPosition\n\t15.\tEmail\n\t16.\tPhone\n\t17.\tComments\n\t18.\tStatus\n\nInstructions:\n\t1.\tuse the provided information to extract the required details.\n\t2.\tOrganize the extracted data into a JSON object where each category is a key and the corresponding extracted information is its value.\n\t3.\tIf a category is not found, use Google searches, whatever resource you want to find the correct information, then if all else fails return null for that category.\n\t4.\tFormat the output as JSON, ensuring it is easy to load and process using Python.\n        5. if there are multiple values in one category, separate the values by a comma ','\n\nOutput Example:\n\n{\n  \"Company name (EN)\": \"ABC Corp\",\n  \"Company name (Local Language if any)\": \"\u682a\u5f0f\u4f1a\u793eABC\",\n  \"Company Website\": \"https://www.abccorp.com\",\n  \"Company owner\": \"John Doe\",\n  \"VET/CRO/BIO/MED/Academia\": \"VET\",\n  \"Country\": \"Japan\",\n  \"LF MRI / HF MRI / CT SCAN\": \"HF MRI\",\n  \"MRI Manufacturer\": \"GE Healthcare\",\n  \"MRI Type\": \"High-field\",\n  \"MRI name\": \"Signa HDx\",\n  \"Civility\": \"Mr.\",\n  \"Contact first name\": \"John\",\n  \"Contact last name\": \"Smith\",\n  \"Position\": \"CEO\",\n  \"Email\": \"john.smith@abccorp.com\",\n  \"Phone\": \"+81 3-1234-5678\",\n  \"Comments\": \"Key stakeholder in HF MRI solutions.\",\n  \"Status\": \"Active\"\n}\n\nNote: Ensure to accurately match HTML elements to these categories, relying on tags, classes, or attributes within the provided HTML. Return an organized JSON object even if not all information is present.\n\nOutput format: JSON as shown above."}, "meta": {"$editor": {"has_automatic_issues": true, "$zap_guesser": {"$ai_field_suggestions": {"choices": {"model": ["gpt-4o-2024-11-20"]}, "fields": {"prompt": ["268045568__body_plain"]}}}}, "parammap": {"model": "gpt-3.5-turbo-instruct"}, "stepTitle": "Total Info Grab"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 268045574, "root_id": 268045568, "action": "send_prompt", "selected_api": "OpenAiCLIAPI@1.19.2", "title": null, "authentication_id": 50832433, "created_at": "2024-11-20T10:16:09+00:00", "last_changed": "2024-11-26T21:05:29+00:00"}, "268764428": {"id": 268764428, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"code": "\nimport json\n\n# Get the input JSON string, defaulting to an empty JSON object if not found\nai_input = input_data.get('AI_input', '{}')\n\nif not ai_input.strip():\n    # Handle empty or whitespace-only input gracefully\n    ai_input = '{}'\n\ntry:\n    # Parse the JSON string into a Python dictionary\n    data = json.loads(ai_input)\nexcept json.JSONDecodeError:\n    # Handle invalid JSON gracefully\n    data = {}  # Default to an empty dictionary\n\n# Define the output dictionary, handling missing keys with default empty strings\noutput = {\n    \"company_name_en\": data.get(\"Company name (EN)\", \"\"),\n    \"company_name_local\": data.get(\"Company name (Local Language if any)\", \"\"),\n    \"company_website\": data.get(\"Company Website\", \"\"),\n    \"company_owner\": data.get(\"Company owner\", \"\"),\n    \"vet_cro_bio_med_academia\": data.get(\"VET/CRO/BIO/MED/Academia\", \"\"),\n    \"country\": data.get(\"Country\", \"\"),\n    \"lf_mri_hf_mri_ct_scan\": data.get(\"LF MRI / HF MRI / CT SCAN\", \"\"),\n    \"mri_manufacturer\": data.get(\"MRI Manufacturer\", \"\"),\n    \"mri_type\": data.get(\"MRI Type\", \"\"),\n    \"mri_name\": data.get(\"MRI name\", \"\"),\n    \"civility\": data.get(\"Civility\", \"\"),\n    \"contact_first_name\": data.get(\"Contact first name\", \"\"),\n    \"contact_last_name\": data.get(\"Contact last name\", \"\"),\n    \"position\": data.get(\"Position\", \"\"),\n    \"email\": data.get(\"Email\", \"\"),\n    \"phone\": data.get(\"Phone\", \"\"),\n    \"comments\": data.get(\"Comments\", \"\"),\n    \"status\": data.get(\"Status\", \"\")\n}\n", "input": {"AI_input": "{{268045575__choices[]text}}"}}, "meta": {"$editor": {"has_automatic_issues": false, "$zap_guesser": {"generated_by_ai": true}}, "parammap": {}}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 268045575, "root_id": 268045568, "action": "01929fad-d3f6-c57c-1b40-ef8691be1f86", "selected_api": "CodeCLIAPI@1.0.1", "title": null, "authentication_id": null, "created_at": "2024-11-25T14:37:28+00:00", "last_changed": "2024-11-26T21:05:29+00:00"}, "268993624": {"id": 268993624, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"storage_source": "one_drive", "folder_id": "01HP6PBRQ3HBO6QF7FOVCKMDW3YKCHI4FK", "workbook_id": "01HP6PBRVTSAHB5DDIORGIQNZYWYFVMKU7", "worksheet_id": "{143CF956-8451-4F14-89A3-1E5A216E8DC7}", "COL$A": ["{{268764428__company_name_en}}, {{268764428__company_name_local}}"], "COL$B": ["{{268764428__company_website}}"], "COL$C": ["{{268764428__company_owner}}"], "COL$D": ["{{268764428__vet_cro_bio_med_academia}}"], "COL$E": ["{{268764428__country}}"], "COL$F": ["{{268764428__lf_mri_hf_mri_ct_scan}}"], "COL$G": ["{{268764428__mri_manufacturer}}"], "COL$H": ["{{268764428__mri_type}}"], "COL$I": ["{{268764428__mri_name}}"], "COL$J": ["{{268764428__civility}}"], "COL$K": ["{{268764428__contact_first_name}}"], "COL$L": ["{{268764428__contact_last_name}}"], "COL$M": ["{{268764428__position}}"], "COL$O": ["{{268764428__phone}}"], "COL$P": ["{{268764428__comments}}"], "COL$Q": ["{{268764428__status}}"], "COL$R": ["{{268045573__choices[]text}}"], "COL$S": ["{{268045574__choices[]text}}"], "COL$T": [], "COL$N": ["{{268764428__email}}"]}, "meta": {"$editor": {"has_automatic_issues": false, "$zap_guesser": {"$ai_field_suggestions": {"fields": {"COL$A": ["268764428__company_name_local"]}}}}, "parammap": {"folder_id": "Zapier Automation", "workbook_id": "DataBase Zap Automation.xlsx", "worksheet_id": "Raw Data Validated by AI"}}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 268764428, "root_id": 268045568, "action": "add_rows", "selected_api": "ExcelCLIAPI@1.4.2", "title": null, "authentication_id": 50800439, "created_at": "2024-11-26T17:55:22+00:00", "last_changed": "2024-11-26T21:05:29+00:00"}}}, {"id": 269024558, "title": "(Copy) Working Linear Simple", "status": "off", "nodes": {"269024558": {"id": 269024558, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "read", "params": {"label_ids": "INBOX"}, "meta": {"$editor": {"$zap_guesser": {"generated_by_ai": false, "zap_guesser_execution_id": "0192e9c9-d0a7-7e64-adf1-7f3c50f4c265", "$ai_field_suggestions": {"choices": {"label_ids": ["CHAT"]}}}, "has_automatic_issues": false}, "parammap": {"label_ids": "INBOX"}, "stepTitle": "New Inbound Email"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": null, "root_id": null, "action": "message", "selected_api": "GoogleMailV2CLIAPI@1.1.11", "title": "(Copy) Working Linear Simple", "authentication_id": 50294643, "created_at": "2024-11-26T20:43:27+00:00", "last_changed": "2024-11-26T20:46:41+00:00"}, "269024559": {"id": 269024559, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"storage_source": "one_drive", "folder_id": "01HP6PBRQ3HBO6QF7FOVCKMDW3YKCHI4FK", "workbook_id": "01HP6PBRVTSAHB5DDIORGIQNZYWYFVMKU7", "worksheet_id": "{00000000-0001-0000-0000-000000000000}", "COL$A": "{{269024558__body_plain}}", "COL$B": "", "COL$C": ""}, "meta": {"$editor": {"has_automatic_issues": false, "$zap_guesser": {"$ai_field_suggestions": {"fields": {"COL$A": ["268045568__body_plain"], "COL$C": ["268045568__from__email", "268045568__body_plain"]}}}}, "parammap": {"folder_id": "Zapier Automation", "workbook_id": "DataBase Zap Automation.xlsx", "worksheet_id": "Links Raw Data"}, "stepTitle": "Add Row"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269024558, "root_id": 269024558, "action": "add_row", "selected_api": "ExcelCLIAPI@1.4.2", "title": "Add Row", "authentication_id": 50800439, "created_at": "2024-11-26T20:43:27+00:00", "last_changed": "2024-11-26T20:46:41+00:00"}, "269024560": {"id": 269024560, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"code": "import requests\nfrom requests.adapters import HTTPAdapter\nfrom urllib.parse import urlparse, urlunparse\nfrom bs4 import BeautifulSoup\n\n# Check urllib3 version to set the correct parameter\ntry:\n    import urllib3\n    urllib3_version = urllib3.__version__\nexcept ImportError:\n    # Default to an older version if urllib3 is not available\n    urllib3_version = '1.25.0'\n\n# Define the Retry class with the appropriate argument\ndef get_retry():\n    from urllib3.util import Retry\n    if urllib3_version >= '1.26.0':\n        return Retry(\n            total=3,\n            backoff_factor=0.3,\n            status_forcelist=[500, 502, 503, 504],\n            allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\", \"POST\"]\n        )\n    else:\n        return Retry(\n            total=3,\n            backoff_factor=0.3,\n            status_forcelist=[500, 502, 503, 504],\n            method_whitelist=[\"HEAD\", \"GET\", \"OPTIONS\", \"POST\"]\n        )\n\n# Get the URL from the input data\nurl = input_data.get('url', '').strip()\n\n# Clean the URL to remove any unwanted characters like '\\r' and '\\n'\nparsed_url = urlparse(url)\nclean_path = parsed_url.path.replace('\\r', '').replace('\\n', '')\nclean_url = urlunparse(parsed_url._replace(path=clean_path))\n\nheaders = {\n    'User-Agent': (\n        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n        '(KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n    )\n}\n\n# Set up a session with retry strategy\nsession = requests.Session()\n\nretry_strategy = get_retry()\nadapter = HTTPAdapter(max_retries=retry_strategy)\nsession.mount('http://', adapter)\nsession.mount('https://', adapter)\n\ntry:\n    # Fetch the HTML content with a timeout\n    response = session.get(clean_url, headers=headers, timeout=5)\n    response.raise_for_status()  # Raise an exception for bad status codes\n\n    # Parse the HTML using BeautifulSoup\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    # Extract the entire content of the <body> tag, including all HTML\n    body_tag = soup.body\n    if body_tag:\n        body_html = str(body_tag)\n    else:\n        body_html = ''\n\n    # Output the body HTML\n    output = {'html': body_html.strip()}\n\nexcept requests.exceptions.RequestException as e:\n    output = {'error': f'Error fetching URL: {e}'}\n\n# For demonstration purposes, print the output\nprint(output)", "input": {"url": "{{269024558__body_plain}}"}}, "meta": {"$editor": {"has_automatic_issues": false, "$zap_guesser": {"generated_by_ai": true}}, "parammap": {}, "stepTitle": "Grab HTML"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269024559, "root_id": 269024558, "action": "01929fad-d3f6-c57c-1b40-ef8691be1f86", "selected_api": "CodeCLIAPI@1.0.1", "title": "Grab HTML", "authentication_id": null, "created_at": "2024-11-26T20:43:27+00:00", "last_changed": "2024-11-26T20:46:41+00:00"}, "269024561": {"id": 269024561, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"code": "import re  # Using the standard 're' module\nimport json\n\n# Enhanced regex patterns compatible with 're' module\npatterns = {\n    'email': r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\",\n\n    'phone': r\"(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{1,4}\\)?[-.\\s]?)*\\d{3,4}\",\n\n    'fax': r\"(?:Fax|F|\u4f20\u771f|\u30d5\u30a1\u30c3\u30af\u30b9|\ud329\uc2a4|\u092b\u0948\u0915\u094d\u0938|\u0442\u0435\u043b|\u0444\u0430\u043a\u0441)[.:]?\\s*(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{1,4}\\)?[-.\\s]?)*\\d{3,4}\",\n\n    'name': r\"\"\"\n        (?:(?:Dr|Mr|Mrs|Ms|Miss|Professor|Prof|MD|DVM|Docteur|V\u00e9t\u00e9rinaire|Asv|\n        \u0938\u0930\u094d\u0935|\u0936\u094d\u0930\u0940|\u0936\u094d\u0930\u0940\u092e\u0924\u0940|Herr|Frau|M\\.|Mme|Mlle|Se\u00f1or|Se\u00f1ora|Se\u00f1orita|\u5148\u751f|\u5973\u58eb|\u6559\u6388|\u535a\u58eb|\uc120\uc0dd\ub2d8|\uad50\uc218\ub2d8|\n        \u0414\u0440|\u0413-\u043d|\u0413-\u0436\u0430|\u0413\u043e\u0441\u043f\u043e\u0434\u0438\u043d|\u0413\u043e\u0441\u043f\u043e\u0436\u0430|\u041f\u0440\u043e\u0444)\\.?\\s+)?\n        (?:[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)+)\n    \"\"\",\n\n    'alt_name': r\"(?:[A-Z][a-z]+(?:\\s+[A-Z][a-z]+){1,2})\",\n\n    'address': r\"\"\"\n        \\d{1,5}\\s+[A-Za-z0-9\\s.,'-]+\n    \"\"\",\n\n    'address_fallback': r\"[A-Za-z0-9\\s.,'-]+\",\n\n    'social_links': {\n        'facebook': r\"(?:https?:\\/\\/)?(?:www\\.)?(?:facebook|fb)\\.(?:com|cn)\\/[^\\/\\s]+\\/?\",\n        'twitter': r\"(?:https?:\\/\\/)?(?:www\\.)?twitter\\.com\\/[^\\/\\s]+\\/?\",\n        'instagram': r\"(?:https?:\\/\\/)?(?:www\\.)?instagram\\.com\\/[^\\/\\s]+\\/?\",\n        'linkedin': r\"(?:https?:\\/\\/)?(?:www\\.)?linkedin\\.com\\/(?:company|in)\\/[^\\/\\s]+\\/?\",\n        'youtube': r\"(?:https?:\\/\\/)?(?:www\\.)?youtube\\.com\\/(?:channel|user)\\/[^\\/\\s]+\\/?\"\n    }\n}\n\n# Compile patterns with appropriate flags\ncompiled_patterns = {\n    'email': re.compile(patterns['email'], re.IGNORECASE),\n    'phone': re.compile(patterns['phone']),\n    'fax': re.compile(patterns['fax'], re.IGNORECASE),\n    'name': re.compile(patterns['name'], re.VERBOSE),\n    'alt_name': re.compile(patterns['alt_name']),\n    'address': re.compile(patterns['address'], re.VERBOSE),\n    'address_fallback': re.compile(patterns['address_fallback']),\n    'social_links': {\n        platform: re.compile(pattern, re.IGNORECASE)\n        for platform, pattern in patterns['social_links'].items()\n    }\n}\n\ndef clean_phone(phone):\n    \"\"\"Clean phone numbers without phonenumbers library\"\"\"\n    try:\n        # Remove all non-digit characters\n        digits = re.sub(r\"\\D\", \"\", phone)\n\n        # Return digits if length is acceptable\n        if len(digits) >= 7:\n            return digits\n        else:\n            return None  # Discard short numbers\n    except Exception:\n        return None\n\ndef is_likely_name(name):\n    \"\"\"Filter out false positives in name detection\"\"\"\n    common_words = {'Home', 'Page', 'Menu', 'Contact', 'About', 'Services', 'Privacy', 'Policy',\n                    'Terms', 'Conditions', 'Search', 'Main', 'Content', 'Navigation', 'Careers',\n                    'Emergency', 'FAQ', 'Log', 'In', 'Out', 'Find', 'More'}\n    words = name.split()\n    return (\n        len(words) >= 2 and\n        not any(word in common_words for word in words) and\n        not any(char.isdigit() for char in name) and\n        all(word[0].isupper() for word in words if word)\n    )\n\ndef extract_info(html):\n    \"\"\"Extract all contact information from HTML\"\"\"\n    try:\n        # Remove style and script contents\n        html_no_style = re.sub(r'<style[^>]*>[\\s\\S]*?</style>', '', html, flags=re.IGNORECASE)\n        html_no_script = re.sub(r'<script[^>]*>[\\s\\S]*?</script>', '', html_no_style, flags=re.IGNORECASE)\n\n        # Convert HTML to text\n        text_content = re.sub(r'<[^>]+>', ' ', html_no_script, flags=re.IGNORECASE)\n        text_content = re.sub(r'\\s+', ' ', text_content, flags=re.IGNORECASE).strip()\n\n        # Extract emails\n        emails = list(set(compiled_patterns['email'].findall(text_content)))\n\n        # Extract and clean phone numbers\n        phones = []\n        phone_matches = compiled_patterns['phone'].findall(text_content)\n        for phone in phone_matches:\n            cleaned = clean_phone(phone)\n            if cleaned and cleaned not in phones:\n                phones.append(cleaned)\n\n        # Extract and clean fax numbers\n        fax_numbers = []\n        fax_contexts = compiled_patterns['fax'].finditer(text_content)\n        for match in fax_contexts:\n            fax = clean_phone(match.group())\n            if fax and fax not in fax_numbers:\n                fax_numbers.append(fax)\n\n        # Extract names\n        names = set()\n        titled_names = compiled_patterns['name'].findall(text_content)\n        names.update(name.strip() for name in titled_names if is_likely_name(name.strip()))\n\n        potential_names = compiled_patterns['alt_name'].findall(text_content)\n        names.update(name.strip() for name in potential_names if is_likely_name(name.strip()))\n\n        # Extract addresses using primary pattern\n        addresses = list(set(compiled_patterns['address'].findall(text_content)))\n\n        # Filter out irrelevant addresses\n        addresses = [addr.strip() for addr in addresses if len(addr.strip().split()) > 2]\n\n        # Extract all href attributes\n        href_pattern = re.compile(r'href=[\\'\"]?([^\\'\" >]+)', re.IGNORECASE)\n        hrefs = href_pattern.findall(html)\n\n        # Extract social media links\n        social_links = {}\n        for platform, pattern in compiled_patterns['social_links'].items():\n            matches = [href for href in hrefs if pattern.search(href)]\n            if matches:\n                social_links[platform] = list(set(matches))\n\n        return {\n            'emails': emails,\n            'phones': phones,\n            'fax': fax_numbers,\n            'names': list(names),\n            'addresses': addresses,\n            'social_links': social_links\n        }\n\n    except Exception as e:\n        print(f\"Error in extraction: {str(e)}\")\n        return {\n            'emails': [],\n            'phones': [],\n            'fax': [],\n            'names': [],\n            'addresses': [],\n            'social_links': {}\n        }\n\n# Get HTML content from input_data provided by Zapier\nhtml_content = input_data.get('html', '')\n\n# Process HTML and prepare output\ntry:\n    if not html_content:\n        output = {\n            'success': False,\n            'error': 'No HTML content provided',\n            'data': None\n        }\n    else:\n        result = extract_info(html_content)\n        output = {\n            'success': True,\n            'error': None,\n            'data': result\n        }\n\nexcept Exception as e:\n    output = {\n        'success': False,\n        'error': str(e),\n        'data': None\n    }\n\n# Print the final output with required ID\nprint(json.dumps({\n    'output': output,\n    'id': '5fgunBIH5nfcaPgwiymwcjuT28AwOIJs'\n}))", "input": {"html": "{{269024560__html}}"}}, "meta": {"$editor": {"has_automatic_issues": false, "$zap_guesser": {"generated_by_ai": true}}, "parammap": {}, "stepTitle": "Parse the HTML"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269024560, "root_id": 269024558, "action": "01929fad-d3f6-c57c-1b40-ef8691be1f86", "selected_api": "CodeCLIAPI@1.0.1", "title": "Parse the HTML", "authentication_id": null, "created_at": "2024-11-26T20:43:27+00:00", "last_changed": "2024-11-26T20:46:41+00:00"}, "269024562": {"id": 269024562, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"storage_source": "one_drive", "folder_id": "01HP6PBRQ3HBO6QF7FOVCKMDW3YKCHI4FK", "workbook_id": "01HP6PBRVTSAHB5DDIORGIQNZYWYFVMKU7", "worksheet_id": "{128204DB-4ECC-484D-8165-72F5C76A50C8}", "COL$A": ["{{269024561__data__emails}}"], "COL$B": ["{{269024561__data__phones}}"], "COL$C": ["{{269024561__data__fax}}"], "COL$D": ["{{269024561__data__names}}"], "COL$E": ["{{269024561__data__addresses}}"], "COL$F": ["{{269024561__data__social_links__facebook}}"]}, "meta": {"$editor": {"has_automatic_issues": false, "$zap_guesser": {"$ai_field_suggestions": {"fields": {"COL$A": ["268045568__body_plain"], "COL$B": ["268045571__data__phones"], "COL$C": ["268045571__data__fax"], "COL$D": ["268045571__data__names"], "COL$E": ["268045568__body_plain", "268045571__data__addresses"], "COL$F": ["268045571__data__social_links__facebook", "268045571__data__social_links__facebook"]}}}}, "parammap": {"folder_id": "Zapier Automation", "workbook_id": "DataBase Zap Automation.xlsx", "worksheet_id": "Raw Data"}}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269024561, "root_id": 269024558, "action": "add_rows", "selected_api": "ExcelCLIAPI@1.4.2", "title": null, "authentication_id": 50800439, "created_at": "2024-11-26T20:43:27+00:00", "last_changed": "2024-11-26T20:46:41+00:00"}, "269024563": {"id": 269024563, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"model": "gpt-3.5-turbo-instruct", "temperature": "0.7", "top_p": "1", "frequency_penalty": "0", "presence_penalty": "0", "prompt": "I have this parsed data from a website - \n{{269024561__data__names}}\n{{269024561__data__addresses}}\n{{269024561__data__social_links__instagram}}\n{{269024561__data__social_links__facebook}}\nI want you to make sense of this information - look up the information online, and have a write-up about the institution this information is related to. \nSee if you can also find names of people associated with this institution. \nThe output should be text. "}, "meta": {"$editor": {"has_automatic_issues": true, "$zap_guesser": {"$ai_field_suggestions": {"choices": {"model": ["gpt-3.5-turbo-instruct"]}}}}, "parammap": {"model": "gpt-3.5-turbo-instruct"}, "stepTitle": "Find General Information"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269024562, "root_id": 269024558, "action": "send_prompt", "selected_api": "OpenAiCLIAPI@1.19.2", "title": "Find General Information", "authentication_id": 50832433, "created_at": "2024-11-26T20:43:27+00:00", "last_changed": "2024-11-26T20:46:41+00:00"}, "269024564": {"id": 269024564, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"model": "gpt-3.5-turbo-instruct", "temperature": "0.7", "top_p": "1", "frequency_penalty": "0", "presence_penalty": "0", "prompt": "I have this list of parsed information from a website - \n{{269024561__data__addresses}}\n\n{{269024561__data__social_links__facebook}}\n{{269024561__data__social_links__instagram}}\n{{269024561__data__names}}\nI want you to make sense of the information - use online searches. \nI want you to find the address and validate it using Google searches, or by navigating to the links above. \nThen output just the validated address. \nThere can be no other answers than the validated address. Use the internet. \nThe output should be just the address of the business in that locality's format"}, "meta": {"$editor": {"has_automatic_issues": true}, "parammap": {"model": "gpt-3.5-turbo-instruct"}, "stepTitle": "Address Validation by AI"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269024563, "root_id": 269024558, "action": "send_prompt", "selected_api": "OpenAiCLIAPI@1.19.2", "title": "Address Validation by AI", "authentication_id": 50832433, "created_at": "2024-11-26T20:43:27+00:00", "last_changed": "2024-11-26T20:46:41+00:00"}, "269024565": {"id": 269024565, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"model": "gpt-3.5-turbo-instruct", "temperature": "0.7", "top_p": "1", "frequency_penalty": "0", "presence_penalty": "0", "prompt": "This is an extract from a parsed HTML (provided information) - it could include links, names, keywords, phone numbers, emails, social media accounts, etc. \n{{269024558__body_plain}}\n{{269024561__data__social_links__instagram}}\n{{269024561__data__social_links__facebook}}\n{{269024561__data__phones}}\n{{269024561__data__fax}}\n{{269024561__data__emails}}\n{{269024561__data__names}}\n{{269024561__data__addresses}}\n\nYour task is to extract, organize, and validate information using the internet based on the following categories:\n\t1.\tCompany name (EN)\n\t2.\tCompany name (Local Language if any)\n\t3.\tCompany Website\n\t4.\tCompany owner\n\t5.\tVET/CRO/BIO/MED/Academia\n\t6.\tCountry\n\t7.\tLF MRI / HF MRI / CT SCAN\n\t8.\tMRI Manufacturer\n\t9.\tMRI Type\n\t10.\tMRI name\n\t11.\tCivility\n\t12.\tContact first name\n\t13.\tContact last name\n\t14.\tPosition\n\t15.\tEmail\n\t16.\tPhone\n\t17.\tComments\n\t18.\tStatus\n\nInstructions:\n\t1.\tuse the provided information to extract the required details.\n\t2.\tOrganize the extracted data into a JSON object where each category is a key and the corresponding extracted information is its value.\n\t3.\tIf a category is not found, use Google searches, whatever resource you want to find the correct information, then if all else fails return null for that category.\n\t4.\tFormat the output as JSON, ensuring it is easy to load and process using Python.\n        5. if there are multiple values in one category, separate the values by a comma ','\n\nOutput Example:\n\n{\n  \"Company name (EN)\": \"ABC Corp\",\n  \"Company name (Local Language if any)\": \"\u682a\u5f0f\u4f1a\u793eABC\",\n  \"Company Website\": \"https://www.abccorp.com\",\n  \"Company owner\": \"John Doe\",\n  \"VET/CRO/BIO/MED/Academia\": \"VET\",\n  \"Country\": \"Japan\",\n  \"LF MRI / HF MRI / CT SCAN\": \"HF MRI\",\n  \"MRI Manufacturer\": \"GE Healthcare\",\n  \"MRI Type\": \"High-field\",\n  \"MRI name\": \"Signa HDx\",\n  \"Civility\": \"Mr.\",\n  \"Contact first name\": \"John\",\n  \"Contact last name\": \"Smith\",\n  \"Position\": \"CEO\",\n  \"Email\": \"john.smith@abccorp.com\",\n  \"Phone\": \"+81 3-1234-5678\",\n  \"Comments\": \"Key stakeholder in HF MRI solutions.\",\n  \"Status\": \"Active\"\n}\n\nNote: Ensure to accurately match HTML elements to these categories, relying on tags, classes, or attributes within the provided HTML. Return an organized JSON object even if not all information is present.\n\nOutput format: JSON as shown above."}, "meta": {"$editor": {"has_automatic_issues": true, "$zap_guesser": {"$ai_field_suggestions": {"choices": {"model": ["gpt-4o-2024-11-20"]}, "fields": {"prompt": ["268045568__body_plain"]}}}}, "parammap": {"model": "gpt-3.5-turbo-instruct"}, "stepTitle": "Total Info Grab"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269024564, "root_id": 269024558, "action": "send_prompt", "selected_api": "OpenAiCLIAPI@1.19.2", "title": "Total Info Grab", "authentication_id": 50832433, "created_at": "2024-11-26T20:43:27+00:00", "last_changed": "2024-11-26T20:46:41+00:00"}, "269024744": {"id": 269024744, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {}, "meta": {"$editor": {"has_automatic_issues": false}}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269024565, "root_id": 269024558, "action": "branch", "selected_api": "BranchingAPI", "title": null, "authentication_id": null, "created_at": "2024-11-26T20:44:50+00:00", "last_changed": "2024-11-26T20:46:41+00:00"}, "269024745": {"id": 269024745, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "filter", "params": {"color": "Heroine Blue", "emoji": "A", "path_eval_stage": 1, "filter_criteria": [{"id": 1026865603023071, "group": 7936573512246387, "key": "", "value": "", "action": "continue"}], "isAlwaysRun": true, "formerTitle": null, "path_eval_index": 0}, "meta": {"$editor": {"has_automatic_issues": false}, "stepTitle": "Path A"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269024744, "root_id": 269024558, "action": "filter", "selected_api": "BranchingAPI", "title": "Path A", "authentication_id": null, "created_at": "2024-11-26T20:44:50+00:00", "last_changed": "2024-11-26T20:46:41+00:00"}, "269024746": {"id": 269024746, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "filter", "params": {"color": "Peru", "emoji": "B", "path_eval_index": 1, "path_eval_stage": 1, "filter_criteria": [{"id": 7499953054020895, "group": 1452536087949563, "key": "", "value": "", "action": "continue"}], "isAlwaysRun": true, "formerTitle": null}, "meta": {"$editor": {"has_automatic_issues": false}, "stepTitle": "Path B"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269024744, "root_id": 269024558, "action": "filter", "selected_api": "BranchingAPI", "title": "Path B", "authentication_id": null, "created_at": "2024-11-26T20:44:50+00:00", "last_changed": "2024-11-26T20:46:41+00:00"}, "269024747": {"id": 269024747, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {}, "meta": {"$editor": {"has_automatic_issues": true}}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269024746, "root_id": 269024558, "action": "", "selected_api": "CodeCLIAPI@1.0.1", "title": null, "authentication_id": null, "created_at": "2024-11-26T20:44:50+00:00", "last_changed": "2024-11-26T20:46:41+00:00"}, "269024896": {"id": 269024896, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {}, "meta": {"$editor": {"has_automatic_issues": true}}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269024747, "root_id": 269024558, "action": "", "selected_api": "VisualStudioOnlineCLIAPI@4.0.1", "title": null, "authentication_id": null, "created_at": "2024-11-26T20:45:54+00:00", "last_changed": "2024-11-26T20:46:41+00:00"}, "269024566": {"id": 269024566, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"code": "\nimport json\n\n# Get the input JSON string\nai_input = input_data.get('AI_input', '{}')\n\n# Parse the JSON string into a Python dictionary\ndata = json.loads(ai_input)\n\n# Prepare the output dictionary, handling null values appropriately\noutput = {\n    \"company_name_en\": data.get(\"Company name (EN)\", \"\"),\n    \"company_name_local\": data.get(\"Company name (Local Language if any)\", \"\"),\n    \"company_website\": data.get(\"Company Website\", \"\"),\n    \"company_owner\": data.get(\"Company owner\", \"\"),\n    \"vet_cro_bio_med_academia\": data.get(\"VET/CRO/BIO/MED/Academia\", \"\"),\n    \"country\": data.get(\"Country\", \"\"),\n    \"lf_mri_hf_mri_ct_scan\": data.get(\"LF MRI / HF MRI / CT SCAN\", \"\"),\n    \"mri_manufacturer\": data.get(\"MRI Manufacturer\", \"\"),\n    \"mri_type\": data.get(\"MRI Type\", \"\"),\n    \"mri_name\": data.get(\"MRI name\", \"\"),\n    \"civility\": data.get(\"Civility\", \"\"),\n    \"contact_first_name\": data.get(\"Contact first name\", \"\"),\n    \"contact_last_name\": data.get(\"Contact last name\", \"\"),\n    \"position\": data.get(\"Position\", \"\"),\n    \"email\": data.get(\"Email\", \"\"),\n    \"phone\": data.get(\"Phone\", \"\"),\n    \"comments\": data.get(\"Comments\", \"\"),\n    \"status\": data.get(\"Status\", \"\")\n}\n", "input": {"AI_input": "{{269024565__choices[]text}}"}}, "meta": {"$editor": {"has_automatic_issues": false, "$zap_guesser": {"generated_by_ai": true}}, "parammap": {}}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269024745, "root_id": 269024558, "action": "01929fad-d3f6-c57c-1b40-ef8691be1f86", "selected_api": "CodeCLIAPI@1.0.1", "title": null, "authentication_id": null, "created_at": "2024-11-26T20:43:27+00:00", "last_changed": "2024-11-26T20:46:41+00:00"}, "269024567": {"id": 269024567, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"storage_source": "one_drive", "folder_id": "01HP6PBRQ3HBO6QF7FOVCKMDW3YKCHI4FK", "workbook_id": "01HP6PBRVTSAHB5DDIORGIQNZYWYFVMKU7", "worksheet_id": "{143CF956-8451-4F14-89A3-1E5A216E8DC7}", "COL$A": ["{{269024566__company_name_en}}, {{269024566__company_name_local}}"], "COL$B": ["{{269024566__company_website}}"], "COL$C": ["{{269024566__company_owner}}"], "COL$D": ["{{269024566__vet_cro_bio_med_academia}}"], "COL$E": ["{{269024566__country}}"], "COL$F": ["{{269024566__lf_mri_hf_mri_ct_scan}}"], "COL$G": ["{{269024566__mri_manufacturer}}"], "COL$H": ["{{269024566__mri_type}}"], "COL$I": ["{{269024566__mri_name}}"], "COL$J": ["{{269024566__civility}}"], "COL$K": ["{{269024566__contact_first_name}}"], "COL$L": ["{{269024566__contact_last_name}}"], "COL$M": ["{{269024566__position}}"], "COL$O": ["{{269024566__phone}}"], "COL$P": ["{{269024566__comments}}"], "COL$Q": ["{{269024566__status}}"], "COL$R": ["{{269024563__choices[]text}}"], "COL$S": ["{{269024564__choices[]text}}"], "COL$T": [], "COL$N": ["{{269024566__email}}"]}, "meta": {"$editor": {"has_automatic_issues": false, "$zap_guesser": {"$ai_field_suggestions": {"fields": {"COL$A": ["268764428__company_name_local"]}}}}, "parammap": {"folder_id": "Zapier Automation", "workbook_id": "DataBase Zap Automation.xlsx", "worksheet_id": "Raw Data Validated by AI"}}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269024566, "root_id": 269024558, "action": "add_rows", "selected_api": "ExcelCLIAPI@1.4.2", "title": null, "authentication_id": 50800439, "created_at": "2024-11-26T20:43:27+00:00", "last_changed": "2024-11-26T20:46:41+00:00"}}}, {"id": 269094705, "title": "EMEA Working Linear Simple", "status": "off", "nodes": {"269094705": {"id": 269094705, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "read", "params": {"label_ids": "INBOX"}, "meta": {"$editor": {"$zap_guesser": {"generated_by_ai": false, "zap_guesser_execution_id": "0192e9c9-d0a7-7e64-adf1-7f3c50f4c265", "$ai_field_suggestions": {"choices": {"label_ids": ["CHAT"]}}}, "has_automatic_issues": false}, "parammap": {"label_ids": "INBOX"}, "stepTitle": "New Inbound Email"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": null, "root_id": null, "action": "message", "selected_api": "GoogleMailV2CLIAPI@1.1.11", "title": "EMEA Working Linear Simple", "authentication_id": 50294643, "created_at": "2024-11-27T09:14:10+00:00", "last_changed": "2024-11-28T10:46:29+00:00"}, "269094706": {"id": 269094706, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"storage_source": "one_drive", "folder_id": "01HP6PBRQ3HBO6QF7FOVCKMDW3YKCHI4FK", "workbook_id": "01HP6PBRQSIAUD5HUBPJBJWKASNWMVCXS2", "worksheet_id": "{00000000-0001-0000-0000-000000000000}", "COL$A": "{{269094705__body_plain}}"}, "meta": {"$editor": {"has_automatic_issues": false, "$zap_guesser": {"$ai_field_suggestions": {"fields": {"COL$A": ["268045568__body_plain"], "COL$C": ["268045568__from__email", "268045568__body_plain"]}}}}, "parammap": {"folder_id": "Zapier Automation", "workbook_id": "Auto AI Mapping EMEA.xlsx", "worksheet_id": "Links Raw Data"}, "stepTitle": "Add Row"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269094705, "root_id": 269094705, "action": "add_row", "selected_api": "ExcelCLIAPI@1.4.2", "title": "Add Row", "authentication_id": 50800439, "created_at": "2024-11-27T09:14:10+00:00", "last_changed": "2024-11-28T10:17:31+00:00"}, "269094707": {"id": 269094707, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"code": "import requests\nfrom requests.adapters import HTTPAdapter\nfrom urllib.parse import urlparse, urlunparse\nfrom bs4 import BeautifulSoup\n\n# Check urllib3 version to set the correct parameter\ntry:\n    import urllib3\n    urllib3_version = urllib3.__version__\nexcept ImportError:\n    # Default to an older version if urllib3 is not available\n    urllib3_version = '1.25.0'\n\n# Define the Retry class with the appropriate argument\ndef get_retry():\n    from urllib3.util import Retry\n    if urllib3_version >= '1.26.0':\n        return Retry(\n            total=3,\n            backoff_factor=0.3,\n            status_forcelist=[500, 502, 503, 504],\n            allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\", \"POST\"]\n        )\n    else:\n        return Retry(\n            total=3,\n            backoff_factor=0.3,\n            status_forcelist=[500, 502, 503, 504],\n            method_whitelist=[\"HEAD\", \"GET\", \"OPTIONS\", \"POST\"]\n        )\n\n# Get the URL from the input data\nurl = input_data.get('url', '').strip()\n\n# Clean the URL to remove any unwanted characters like '\\r' and '\\n'\nparsed_url = urlparse(url)\nclean_path = parsed_url.path.replace('\\r', '').replace('\\n', '')\nclean_url = urlunparse(parsed_url._replace(path=clean_path))\n\nheaders = {\n    'User-Agent': (\n        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n        '(KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n    )\n}\n\n# Set up a session with retry strategy\nsession = requests.Session()\n\nretry_strategy = get_retry()\nadapter = HTTPAdapter(max_retries=retry_strategy)\nsession.mount('http://', adapter)\nsession.mount('https://', adapter)\n\ntry:\n    # Fetch the HTML content with a timeout\n    response = session.get(clean_url, headers=headers, timeout=5)\n    response.raise_for_status()  # Raise an exception for bad status codes\n\n    # Parse the HTML using BeautifulSoup\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    # Extract the entire content of the <body> tag, including all HTML\n    body_tag = soup.body\n    if body_tag:\n        body_html = str(body_tag)\n    else:\n        body_html = ''\n\n    # Output the body HTML\n    output = {'html': body_html.strip()}\n\nexcept requests.exceptions.RequestException as e:\n    output = {'error': f'Error fetching URL: {e}'}\n\n# For demonstration purposes, print the output\nprint(output)", "input": {"url": "{{269094705__body_plain}}"}}, "meta": {"$editor": {"has_automatic_issues": false, "$zap_guesser": {"generated_by_ai": true}}, "parammap": {}, "stepTitle": "Grab HTML"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269094706, "root_id": 269094705, "action": "01929fad-d3f6-c57c-1b40-ef8691be1f86", "selected_api": "CodeCLIAPI@1.0.1", "title": "Grab HTML", "authentication_id": null, "created_at": "2024-11-27T09:14:10+00:00", "last_changed": "2024-11-28T10:17:31+00:00"}, "269094708": {"id": 269094708, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"code": "import re  # Using the standard 're' module\nimport json\n\n# Enhanced regex patterns compatible with 're' module\npatterns = {\n    'email': r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\",\n\n    'phone': r\"(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{1,4}\\)?[-.\\s]?)*\\d{3,4}\",\n\n    'fax': r\"(?:Fax|F|\u4f20\u771f|\u30d5\u30a1\u30c3\u30af\u30b9|\ud329\uc2a4|\u092b\u0948\u0915\u094d\u0938|\u0442\u0435\u043b|\u0444\u0430\u043a\u0441)[.:]?\\s*(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{1,4}\\)?[-.\\s]?)*\\d{3,4}\",\n\n    'name': r\"\"\"\n        (?:(?:Dr|Mr|Mrs|Ms|Miss|Professor|Prof|MD|DVM|Docteur|V\u00e9t\u00e9rinaire|Asv|\n        \u0938\u0930\u094d\u0935|\u0936\u094d\u0930\u0940|\u0936\u094d\u0930\u0940\u092e\u0924\u0940|Herr|Frau|M\\.|Mme|Mlle|Se\u00f1or|Se\u00f1ora|Se\u00f1orita|\u5148\u751f|\u5973\u58eb|\u6559\u6388|\u535a\u58eb|\uc120\uc0dd\ub2d8|\uad50\uc218\ub2d8|\n        \u0414\u0440|\u0413-\u043d|\u0413-\u0436\u0430|\u0413\u043e\u0441\u043f\u043e\u0434\u0438\u043d|\u0413\u043e\u0441\u043f\u043e\u0436\u0430|\u041f\u0440\u043e\u0444)\\.?\\s+)?\n        (?:[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)+)\n    \"\"\",\n\n    'alt_name': r\"(?:[A-Z][a-z]+(?:\\s+[A-Z][a-z]+){1,2})\",\n\n    'address': r\"\"\"\n        \\d{1,5}\\s+[A-Za-z0-9\\s.,'-]+\n    \"\"\",\n\n    'address_fallback': r\"[A-Za-z0-9\\s.,'-]+\",\n\n    'social_links': {\n        'facebook': r\"(?:https?:\\/\\/)?(?:www\\.)?(?:facebook|fb)\\.(?:com|cn)\\/[^\\/\\s]+\\/?\",\n        'twitter': r\"(?:https?:\\/\\/)?(?:www\\.)?twitter\\.com\\/[^\\/\\s]+\\/?\",\n        'instagram': r\"(?:https?:\\/\\/)?(?:www\\.)?instagram\\.com\\/[^\\/\\s]+\\/?\",\n        'linkedin': r\"(?:https?:\\/\\/)?(?:www\\.)?linkedin\\.com\\/(?:company|in)\\/[^\\/\\s]+\\/?\",\n        'youtube': r\"(?:https?:\\/\\/)?(?:www\\.)?youtube\\.com\\/(?:channel|user)\\/[^\\/\\s]+\\/?\"\n    }\n}\n\n# Compile patterns with appropriate flags\ncompiled_patterns = {\n    'email': re.compile(patterns['email'], re.IGNORECASE),\n    'phone': re.compile(patterns['phone']),\n    'fax': re.compile(patterns['fax'], re.IGNORECASE),\n    'name': re.compile(patterns['name'], re.VERBOSE),\n    'alt_name': re.compile(patterns['alt_name']),\n    'address': re.compile(patterns['address'], re.VERBOSE),\n    'address_fallback': re.compile(patterns['address_fallback']),\n    'social_links': {\n        platform: re.compile(pattern, re.IGNORECASE)\n        for platform, pattern in patterns['social_links'].items()\n    }\n}\n\ndef clean_phone(phone):\n    \"\"\"Clean phone numbers without phonenumbers library\"\"\"\n    try:\n        # Remove all non-digit characters\n        digits = re.sub(r\"\\D\", \"\", phone)\n\n        # Return digits if length is acceptable\n        if len(digits) >= 7:\n            return digits\n        else:\n            return None  # Discard short numbers\n    except Exception:\n        return None\n\ndef is_likely_name(name):\n    \"\"\"Filter out false positives in name detection\"\"\"\n    common_words = {'Home', 'Page', 'Menu', 'Contact', 'About', 'Services', 'Privacy', 'Policy',\n                    'Terms', 'Conditions', 'Search', 'Main', 'Content', 'Navigation', 'Careers',\n                    'Emergency', 'FAQ', 'Log', 'In', 'Out', 'Find', 'More'}\n    words = name.split()\n    return (\n        len(words) >= 2 and\n        not any(word in common_words for word in words) and\n        not any(char.isdigit() for char in name) and\n        all(word[0].isupper() for word in words if word)\n    )\n\ndef extract_info(html):\n    \"\"\"Extract all contact information from HTML\"\"\"\n    try:\n        # Remove style and script contents\n        html_no_style = re.sub(r'<style[^>]*>[\\s\\S]*?</style>', '', html, flags=re.IGNORECASE)\n        html_no_script = re.sub(r'<script[^>]*>[\\s\\S]*?</script>', '', html_no_style, flags=re.IGNORECASE)\n\n        # Convert HTML to text\n        text_content = re.sub(r'<[^>]+>', ' ', html_no_script, flags=re.IGNORECASE)\n        text_content = re.sub(r'\\s+', ' ', text_content, flags=re.IGNORECASE).strip()\n\n        # Extract emails\n        emails = list(set(compiled_patterns['email'].findall(text_content)))\n\n        # Extract and clean phone numbers\n        phones = []\n        phone_matches = compiled_patterns['phone'].findall(text_content)\n        for phone in phone_matches:\n            cleaned = clean_phone(phone)\n            if cleaned and cleaned not in phones:\n                phones.append(cleaned)\n\n        # Extract and clean fax numbers\n        fax_numbers = []\n        fax_contexts = compiled_patterns['fax'].finditer(text_content)\n        for match in fax_contexts:\n            fax = clean_phone(match.group())\n            if fax and fax not in fax_numbers:\n                fax_numbers.append(fax)\n\n        # Extract names\n        names = set()\n        titled_names = compiled_patterns['name'].findall(text_content)\n        names.update(name.strip() for name in titled_names if is_likely_name(name.strip()))\n\n        potential_names = compiled_patterns['alt_name'].findall(text_content)\n        names.update(name.strip() for name in potential_names if is_likely_name(name.strip()))\n\n        # Extract addresses using primary pattern\n        addresses = list(set(compiled_patterns['address'].findall(text_content)))\n\n        # Filter out irrelevant addresses\n        addresses = [addr.strip() for addr in addresses if len(addr.strip().split()) > 2]\n\n        # Extract all href attributes\n        href_pattern = re.compile(r'href=[\\'\"]?([^\\'\" >]+)', re.IGNORECASE)\n        hrefs = href_pattern.findall(html)\n\n        # Extract social media links\n        social_links = {}\n        for platform, pattern in compiled_patterns['social_links'].items():\n            matches = [href for href in hrefs if pattern.search(href)]\n            if matches:\n                social_links[platform] = list(set(matches))\n\n        return {\n            'emails': emails,\n            'phones': phones,\n            'fax': fax_numbers,\n            'names': list(names),\n            'addresses': addresses,\n            'social_links': social_links\n        }\n\n    except Exception as e:\n        print(f\"Error in extraction: {str(e)}\")\n        return {\n            'emails': [],\n            'phones': [],\n            'fax': [],\n            'names': [],\n            'addresses': [],\n            'social_links': {}\n        }\n\n# Get HTML content from input_data provided by Zapier\nhtml_content = input_data.get('html', '')\n\n# Process HTML and prepare output\ntry:\n    if not html_content:\n        output = {\n            'success': False,\n            'error': 'No HTML content provided',\n            'data': None\n        }\n    else:\n        result = extract_info(html_content)\n        output = {\n            'success': True,\n            'error': None,\n            'data': result\n        }\n\nexcept Exception as e:\n    output = {\n        'success': False,\n        'error': str(e),\n        'data': None\n    }\n\n# Print the final output with required ID\nprint(json.dumps({\n    'output': output,\n    'id': '5fgunBIH5nfcaPgwiymwcjuT28AwOIJs'\n}))", "input": {"html": "{{269094707__html}}"}}, "meta": {"$editor": {"has_automatic_issues": false, "$zap_guesser": {"generated_by_ai": true}}, "parammap": {}, "stepTitle": "Parse the HTML"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269094707, "root_id": 269094705, "action": "01929fad-d3f6-c57c-1b40-ef8691be1f86", "selected_api": "CodeCLIAPI@1.0.1", "title": "Parse the HTML", "authentication_id": null, "created_at": "2024-11-27T09:14:10+00:00", "last_changed": "2024-11-28T10:17:31+00:00"}, "269094709": {"id": 269094709, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"storage_source": "one_drive", "folder_id": "01HP6PBRQ3HBO6QF7FOVCKMDW3YKCHI4FK", "workbook_id": "01HP6PBRQSIAUD5HUBPJBJWKASNWMVCXS2", "worksheet_id": "{4AF3F056-9033-40F1-BB8A-4C52183B62EA}", "COL$A": ["{{269094708__data__emails}}"], "COL$B": ["{{269094708__data__phones}}"], "COL$C": ["{{269094708__data__fax}}"], "COL$D": ["{{269094708__data__names}}"], "COL$E": ["{{269094708__data__addresses}}"], "COL$F": ["{{269094708__data__social_links__facebook}}"]}, "meta": {"$editor": {"has_automatic_issues": false, "$zap_guesser": {"$ai_field_suggestions": {"fields": {"COL$A": ["268045568__body_plain"], "COL$B": ["268045571__data__phones"], "COL$C": ["268045571__data__fax"], "COL$D": ["268045571__data__names"], "COL$E": ["268045568__body_plain", "268045571__data__addresses"], "COL$F": ["268045571__data__social_links__facebook", "268045571__data__social_links__facebook"]}}}}, "parammap": {"folder_id": "Zapier Automation", "workbook_id": "Auto AI Mapping EMEA.xlsx", "worksheet_id": "Raw Data"}}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269094708, "root_id": 269094705, "action": "add_rows", "selected_api": "ExcelCLIAPI@1.4.2", "title": null, "authentication_id": 50800439, "created_at": "2024-11-27T09:14:10+00:00", "last_changed": "2024-11-28T10:17:31+00:00"}, "269094710": {"id": 269094710, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"model": "gpt-3.5-turbo-instruct", "temperature": "0.7", "top_p": "1", "frequency_penalty": "0", "presence_penalty": "0", "prompt": "I have this parsed data from a website - \n{{269094708__data__names}}\n{{269094708__data__addresses}}\n{{269094708__data__social_links__instagram}}\n{{269094708__data__social_links__facebook}}\nI want you to make sense of this information - look up the information online, and have a write-up about the institution this information is related to. \nSee if you can also find names of people associated with this institution. \nThe output should be text. "}, "meta": {"$editor": {"has_automatic_issues": true, "$zap_guesser": {"$ai_field_suggestions": {"choices": {"model": ["gpt-3.5-turbo-instruct"]}}}}, "parammap": {"model": "gpt-3.5-turbo-instruct"}, "stepTitle": "Find General Information"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269094709, "root_id": 269094705, "action": "send_prompt", "selected_api": "OpenAiCLIAPI@1.19.2", "title": "Find General Information", "authentication_id": 50832433, "created_at": "2024-11-27T09:14:10+00:00", "last_changed": "2024-11-28T10:17:31+00:00"}, "269094711": {"id": 269094711, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"model": "gpt-3.5-turbo-instruct", "temperature": "0.7", "top_p": "1", "frequency_penalty": "0", "presence_penalty": "0", "prompt": "I have this list of parsed information from a website - \n{{269094708__data__addresses}}\n\n{{269094708__data__social_links__facebook}}\n{{269094708__data__social_links__instagram}}\n{{269094708__data__names}}\nI want you to make sense of the information - use online searches. \nI want you to find the address and validate it using Google searches, or by navigating to the links above. \nThen output just the validated address. \nThere can be no other answers than the validated address. Use the internet. \nThe output should be just the address of the business in that locality's format"}, "meta": {"$editor": {"has_automatic_issues": true}, "parammap": {"model": "gpt-3.5-turbo-instruct"}, "stepTitle": "Address Validation by AI"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269094710, "root_id": 269094705, "action": "send_prompt", "selected_api": "OpenAiCLIAPI@1.19.2", "title": "Address Validation by AI", "authentication_id": 50832433, "created_at": "2024-11-27T09:14:10+00:00", "last_changed": "2024-11-28T10:17:31+00:00"}, "269094712": {"id": 269094712, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"model": "gpt-3.5-turbo-instruct", "temperature": "0.7", "top_p": "1", "frequency_penalty": "0", "presence_penalty": "0", "prompt": "This is an extract from a parsed HTML (provided information) - it could include links, names, keywords, phone numbers, emails, social media accounts, etc. \n{{269094705__body_plain}}\n{{269094708__data__social_links__instagram}}\n{{269094708__data__social_links__facebook}}\n{{269094708__data__phones}}\n{{269094708__data__fax}}\n{{269094708__data__emails}}\n{{269094708__data__names}}\n{{269094708__data__addresses}}\n\nYour task is to extract, organize, and validate information using the internet based on the following categories:\n\t1.\tCompany name (EN)\n\t2.\tCompany name (Local Language if any)\n\t3.\tCompany Website\n\t4.\tCompany owner\n\t5.\tVET/CRO/BIO/MED/Academia\n\t6.\tCountry\n\t7.\tLF MRI / HF MRI / CT SCAN\n\t8.\tMRI Manufacturer\n\t9.\tMRI Type\n\t10.\tMRI name\n\t11.\tCivility\n\t12.\tContact first name\n\t13.\tContact last name\n\t14.\tPosition\n\t15.\tEmail\n\t16.\tPhone\n\t17.\tComments\n\t18.\tStatus\n\nInstructions:\n\t1.\tuse the provided information to extract the required details.\n\t2.\tOrganize the extracted data into a JSON object where each category is a key and the corresponding extracted information is its value.\n\t3.\tIf a category is not found, use Google searches, whatever resource you want to find the correct information, then if all else fails return null for that category.\n\t4.\tFormat the output as JSON, ensuring it is easy to load and process using Python.\n        5. if there are multiple values in one category, separate the values by a comma ','\n\nOutput Example:\n\n{\n  \"Company name (EN)\": \"ABC Corp\",\n  \"Company name (Local Language if any)\": \"\u682a\u5f0f\u4f1a\u793eABC\",\n  \"Company Website\": \"https://www.abccorp.com\",\n  \"Company owner\": \"John Doe\",\n  \"VET/CRO/BIO/MED/Academia\": \"VET\",\n  \"Country\": \"Japan\",\n  \"LF MRI / HF MRI / CT SCAN\": \"HF MRI\",\n  \"MRI Manufacturer\": \"GE Healthcare\",\n  \"MRI Type\": \"High-field\",\n  \"MRI name\": \"Signa HDx\",\n  \"Civility\": \"Mr.\",\n  \"Contact first name\": \"John\",\n  \"Contact last name\": \"Smith\",\n  \"Position\": \"CEO\",\n  \"Email\": \"john.smith@abccorp.com\",\n  \"Phone\": \"+81 3-1234-5678\",\n  \"Comments\": \"Key stakeholder in HF MRI solutions.\",\n  \"Status\": \"Active\"\n}\n\nNote: Ensure to accurately match HTML elements to these categories, relying on tags, classes, or attributes within the provided HTML. Return an organized JSON object even if not all information is present.\n\nOutput format: JSON as shown above."}, "meta": {"$editor": {"has_automatic_issues": true, "$zap_guesser": {"$ai_field_suggestions": {"choices": {"model": ["gpt-4o-2024-11-20"]}, "fields": {"prompt": ["268045568__body_plain"]}}}}, "parammap": {"model": "gpt-3.5-turbo-instruct"}, "stepTitle": "Total Info Grab"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269094711, "root_id": 269094705, "action": "send_prompt", "selected_api": "OpenAiCLIAPI@1.19.2", "title": "Total Info Grab", "authentication_id": 50832433, "created_at": "2024-11-27T09:14:10+00:00", "last_changed": "2024-11-28T10:17:31+00:00"}, "269094714": {"id": 269094714, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"code": "\nimport json\n\n# Get the input JSON string, defaulting to an empty JSON object if not found\nai_input = input_data.get('AI_input', '{}')\n\nif not ai_input.strip():\n    # Handle empty or whitespace-only input gracefully\n    ai_input = '{}'\n\ntry:\n    # Parse the JSON string into a Python dictionary\n    data = json.loads(ai_input)\nexcept json.JSONDecodeError:\n    # Handle invalid JSON gracefully\n    data = {}  # Default to an empty dictionary\n\n# Define the output dictionary, handling missing keys with default empty strings\noutput = {\n    \"company_name_en\": data.get(\"Company name (EN)\", \"\"),\n    \"company_name_local\": data.get(\"Company name (Local Language if any)\", \"\"),\n    \"company_website\": data.get(\"Company Website\", \"\"),\n    \"company_owner\": data.get(\"Company owner\", \"\"),\n    \"vet_cro_bio_med_academia\": data.get(\"VET/CRO/BIO/MED/Academia\", \"\"),\n    \"country\": data.get(\"Country\", \"\"),\n    \"lf_mri_hf_mri_ct_scan\": data.get(\"LF MRI / HF MRI / CT SCAN\", \"\"),\n    \"mri_manufacturer\": data.get(\"MRI Manufacturer\", \"\"),\n    \"mri_type\": data.get(\"MRI Type\", \"\"),\n    \"mri_name\": data.get(\"MRI name\", \"\"),\n    \"civility\": data.get(\"Civility\", \"\"),\n    \"contact_first_name\": data.get(\"Contact first name\", \"\"),\n    \"contact_last_name\": data.get(\"Contact last name\", \"\"),\n    \"position\": data.get(\"Position\", \"\"),\n    \"email\": data.get(\"Email\", \"\"),\n    \"phone\": data.get(\"Phone\", \"\"),\n    \"comments\": data.get(\"Comments\", \"\"),\n    \"status\": data.get(\"Status\", \"\")\n}\n", "input": {"AI_input": "{{269094712__choices[]text}}"}}, "meta": {"$editor": {"has_automatic_issues": false, "$zap_guesser": {"generated_by_ai": true}}, "parammap": {}}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269094712, "root_id": 269094705, "action": "01929fad-d3f6-c57c-1b40-ef8691be1f86", "selected_api": "CodeCLIAPI@1.0.1", "title": null, "authentication_id": null, "created_at": "2024-11-27T09:14:10+00:00", "last_changed": "2024-11-28T10:17:31+00:00"}, "269094715": {"id": 269094715, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"storage_source": "one_drive", "folder_id": "01HP6PBRQ3HBO6QF7FOVCKMDW3YKCHI4FK", "workbook_id": "01HP6PBRQSIAUD5HUBPJBJWKASNWMVCXS2", "worksheet_id": "{98C0F1C6-9CF4-4A74-B6DF-2607927990FF}", "COL$A": ["{{269094714__company_name_en}}, {{269094714__company_name_local}}"], "COL$B": ["{{269094714__company_website}}"], "COL$C": ["{{269094714__company_owner}}"], "COL$D": ["{{269094714__vet_cro_bio_med_academia}}"], "COL$E": ["{{269094714__country}}"], "COL$F": ["{{269094714__lf_mri_hf_mri_ct_scan}}"], "COL$G": ["{{269094714__mri_manufacturer}}"], "COL$H": ["{{269094714__mri_type}}"], "COL$I": ["{{269094714__mri_name}}"], "COL$J": ["{{269094714__civility}}"], "COL$K": ["{{269094714__contact_first_name}}"], "COL$L": ["{{269094714__contact_last_name}}"], "COL$M": ["{{269094714__position}}"], "COL$O": ["{{269094714__phone}}"], "COL$P": ["{{269094714__comments}}"], "COL$Q": ["{{269094714__status}}"], "COL$R": ["{{269094710__choices[]text}}"], "COL$S": ["{{269094711__choices[]text}}"], "COL$T": [], "COL$N": ["{{269094714__email}}"]}, "meta": {"$editor": {"has_automatic_issues": true, "$zap_guesser": {"$ai_field_suggestions": {"fields": {"COL$A": ["268764428__company_name_local"]}}}}, "parammap": {"folder_id": "Zapier Automation", "workbook_id": "Auto AI Mapping EMEA.xlsx", "worksheet_id": "Raw Data Validated by AI"}}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269094714, "root_id": 269094705, "action": "add_rows", "selected_api": "ExcelCLIAPI@1.4.2", "title": null, "authentication_id": 50800439, "created_at": "2024-11-27T09:14:10+00:00", "last_changed": "2024-11-28T10:17:31+00:00"}}}, {"id": 269094723, "title": "PreClinical Working Linear Simple", "status": "off", "nodes": {"269094723": {"id": 269094723, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "read", "params": {"label_ids": "INBOX"}, "meta": {"$editor": {"$zap_guesser": {"generated_by_ai": false, "zap_guesser_execution_id": "0192e9c9-d0a7-7e64-adf1-7f3c50f4c265", "$ai_field_suggestions": {"choices": {"label_ids": ["CHAT"]}}}, "has_automatic_issues": false}, "parammap": {"label_ids": "INBOX"}, "stepTitle": "New Inbound Email"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": null, "root_id": null, "action": "message", "selected_api": "GoogleMailV2CLIAPI@1.1.11", "title": "PreClinical Working Linear Simple", "authentication_id": 50864343, "created_at": "2024-11-27T09:14:16+00:00", "last_changed": "2024-11-28T10:46:31+00:00"}, "269094724": {"id": 269094724, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"storage_source": "one_drive", "folder_id": "01HP6PBRQ3HBO6QF7FOVCKMDW3YKCHI4FK", "workbook_id": "01HP6PBRSPIHYYUPNLJ5GZOKMV4PMDONQE", "worksheet_id": "{00000000-0001-0000-0000-000000000000}", "COL$A": "{{269094723__body_plain}}"}, "meta": {"$editor": {"has_automatic_issues": false, "$zap_guesser": {"$ai_field_suggestions": {"fields": {"COL$A": ["268045568__body_plain"], "COL$C": ["268045568__from__email", "268045568__body_plain"]}}}}, "parammap": {"folder_id": "Zapier Automation", "workbook_id": "Auto AI Mapping Pre-Clinical.xlsx", "worksheet_id": "Links Raw Data"}, "stepTitle": "Add Row"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269094723, "root_id": 269094723, "action": "add_row", "selected_api": "ExcelCLIAPI@1.4.2", "title": "Add Row", "authentication_id": 50800439, "created_at": "2024-11-27T09:14:16+00:00", "last_changed": "2024-11-27T09:46:45+00:00"}, "269094725": {"id": 269094725, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"code": "import requests\nfrom requests.adapters import HTTPAdapter\nfrom urllib.parse import urlparse, urlunparse\nfrom bs4 import BeautifulSoup\n\n# Check urllib3 version to set the correct parameter\ntry:\n    import urllib3\n    urllib3_version = urllib3.__version__\nexcept ImportError:\n    # Default to an older version if urllib3 is not available\n    urllib3_version = '1.25.0'\n\n# Define the Retry class with the appropriate argument\ndef get_retry():\n    from urllib3.util import Retry\n    if urllib3_version >= '1.26.0':\n        return Retry(\n            total=3,\n            backoff_factor=0.3,\n            status_forcelist=[500, 502, 503, 504],\n            allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\", \"POST\"]\n        )\n    else:\n        return Retry(\n            total=3,\n            backoff_factor=0.3,\n            status_forcelist=[500, 502, 503, 504],\n            method_whitelist=[\"HEAD\", \"GET\", \"OPTIONS\", \"POST\"]\n        )\n\n# Get the URL from the input data\nurl = input_data.get('url', '').strip()\n\n# Clean the URL to remove any unwanted characters like '\\r' and '\\n'\nparsed_url = urlparse(url)\nclean_path = parsed_url.path.replace('\\r', '').replace('\\n', '')\nclean_url = urlunparse(parsed_url._replace(path=clean_path))\n\nheaders = {\n    'User-Agent': (\n        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n        '(KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n    )\n}\n\n# Set up a session with retry strategy\nsession = requests.Session()\n\nretry_strategy = get_retry()\nadapter = HTTPAdapter(max_retries=retry_strategy)\nsession.mount('http://', adapter)\nsession.mount('https://', adapter)\n\ntry:\n    # Fetch the HTML content with a timeout\n    response = session.get(clean_url, headers=headers, timeout=5)\n    response.raise_for_status()  # Raise an exception for bad status codes\n\n    # Parse the HTML using BeautifulSoup\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    # Extract the entire content of the <body> tag, including all HTML\n    body_tag = soup.body\n    if body_tag:\n        body_html = str(body_tag)\n    else:\n        body_html = ''\n\n    # Output the body HTML\n    output = {'html': body_html.strip()}\n\nexcept requests.exceptions.RequestException as e:\n    output = {'error': f'Error fetching URL: {e}'}\n\n# For demonstration purposes, print the output\nprint(output)", "input": {"url": "{{269094723__body_plain}}"}}, "meta": {"$editor": {"has_automatic_issues": false, "$zap_guesser": {"generated_by_ai": true}}, "parammap": {}, "stepTitle": "Grab HTML"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269094724, "root_id": 269094723, "action": "01929fad-d3f6-c57c-1b40-ef8691be1f86", "selected_api": "CodeCLIAPI@1.0.1", "title": "Grab HTML", "authentication_id": null, "created_at": "2024-11-27T09:14:16+00:00", "last_changed": "2024-11-27T09:46:45+00:00"}, "269094726": {"id": 269094726, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"code": "import re  # Using the standard 're' module\nimport json\n\n# Enhanced regex patterns compatible with 're' module\npatterns = {\n    'email': r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\",\n\n    'phone': r\"(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{1,4}\\)?[-.\\s]?)*\\d{3,4}\",\n\n    'fax': r\"(?:Fax|F|\u4f20\u771f|\u30d5\u30a1\u30c3\u30af\u30b9|\ud329\uc2a4|\u092b\u0948\u0915\u094d\u0938|\u0442\u0435\u043b|\u0444\u0430\u043a\u0441)[.:]?\\s*(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{1,4}\\)?[-.\\s]?)*\\d{3,4}\",\n\n    'name': r\"\"\"\n        (?:(?:Dr|Mr|Mrs|Ms|Miss|Professor|Prof|MD|DVM|Docteur|V\u00e9t\u00e9rinaire|Asv|\n        \u0938\u0930\u094d\u0935|\u0936\u094d\u0930\u0940|\u0936\u094d\u0930\u0940\u092e\u0924\u0940|Herr|Frau|M\\.|Mme|Mlle|Se\u00f1or|Se\u00f1ora|Se\u00f1orita|\u5148\u751f|\u5973\u58eb|\u6559\u6388|\u535a\u58eb|\uc120\uc0dd\ub2d8|\uad50\uc218\ub2d8|\n        \u0414\u0440|\u0413-\u043d|\u0413-\u0436\u0430|\u0413\u043e\u0441\u043f\u043e\u0434\u0438\u043d|\u0413\u043e\u0441\u043f\u043e\u0436\u0430|\u041f\u0440\u043e\u0444)\\.?\\s+)?\n        (?:[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)+)\n    \"\"\",\n\n    'alt_name': r\"(?:[A-Z][a-z]+(?:\\s+[A-Z][a-z]+){1,2})\",\n\n    'address': r\"\"\"\n        \\d{1,5}\\s+[A-Za-z0-9\\s.,'-]+\n    \"\"\",\n\n    'address_fallback': r\"[A-Za-z0-9\\s.,'-]+\",\n\n    'social_links': {\n        'facebook': r\"(?:https?:\\/\\/)?(?:www\\.)?(?:facebook|fb)\\.(?:com|cn)\\/[^\\/\\s]+\\/?\",\n        'twitter': r\"(?:https?:\\/\\/)?(?:www\\.)?twitter\\.com\\/[^\\/\\s]+\\/?\",\n        'instagram': r\"(?:https?:\\/\\/)?(?:www\\.)?instagram\\.com\\/[^\\/\\s]+\\/?\",\n        'linkedin': r\"(?:https?:\\/\\/)?(?:www\\.)?linkedin\\.com\\/(?:company|in)\\/[^\\/\\s]+\\/?\",\n        'youtube': r\"(?:https?:\\/\\/)?(?:www\\.)?youtube\\.com\\/(?:channel|user)\\/[^\\/\\s]+\\/?\"\n    }\n}\n\n# Compile patterns with appropriate flags\ncompiled_patterns = {\n    'email': re.compile(patterns['email'], re.IGNORECASE),\n    'phone': re.compile(patterns['phone']),\n    'fax': re.compile(patterns['fax'], re.IGNORECASE),\n    'name': re.compile(patterns['name'], re.VERBOSE),\n    'alt_name': re.compile(patterns['alt_name']),\n    'address': re.compile(patterns['address'], re.VERBOSE),\n    'address_fallback': re.compile(patterns['address_fallback']),\n    'social_links': {\n        platform: re.compile(pattern, re.IGNORECASE)\n        for platform, pattern in patterns['social_links'].items()\n    }\n}\n\ndef clean_phone(phone):\n    \"\"\"Clean phone numbers without phonenumbers library\"\"\"\n    try:\n        # Remove all non-digit characters\n        digits = re.sub(r\"\\D\", \"\", phone)\n\n        # Return digits if length is acceptable\n        if len(digits) >= 7:\n            return digits\n        else:\n            return None  # Discard short numbers\n    except Exception:\n        return None\n\ndef is_likely_name(name):\n    \"\"\"Filter out false positives in name detection\"\"\"\n    common_words = {'Home', 'Page', 'Menu', 'Contact', 'About', 'Services', 'Privacy', 'Policy',\n                    'Terms', 'Conditions', 'Search', 'Main', 'Content', 'Navigation', 'Careers',\n                    'Emergency', 'FAQ', 'Log', 'In', 'Out', 'Find', 'More'}\n    words = name.split()\n    return (\n        len(words) >= 2 and\n        not any(word in common_words for word in words) and\n        not any(char.isdigit() for char in name) and\n        all(word[0].isupper() for word in words if word)\n    )\n\ndef extract_info(html):\n    \"\"\"Extract all contact information from HTML\"\"\"\n    try:\n        # Remove style and script contents\n        html_no_style = re.sub(r'<style[^>]*>[\\s\\S]*?</style>', '', html, flags=re.IGNORECASE)\n        html_no_script = re.sub(r'<script[^>]*>[\\s\\S]*?</script>', '', html_no_style, flags=re.IGNORECASE)\n\n        # Convert HTML to text\n        text_content = re.sub(r'<[^>]+>', ' ', html_no_script, flags=re.IGNORECASE)\n        text_content = re.sub(r'\\s+', ' ', text_content, flags=re.IGNORECASE).strip()\n\n        # Extract emails\n        emails = list(set(compiled_patterns['email'].findall(text_content)))\n\n        # Extract and clean phone numbers\n        phones = []\n        phone_matches = compiled_patterns['phone'].findall(text_content)\n        for phone in phone_matches:\n            cleaned = clean_phone(phone)\n            if cleaned and cleaned not in phones:\n                phones.append(cleaned)\n\n        # Extract and clean fax numbers\n        fax_numbers = []\n        fax_contexts = compiled_patterns['fax'].finditer(text_content)\n        for match in fax_contexts:\n            fax = clean_phone(match.group())\n            if fax and fax not in fax_numbers:\n                fax_numbers.append(fax)\n\n        # Extract names\n        names = set()\n        titled_names = compiled_patterns['name'].findall(text_content)\n        names.update(name.strip() for name in titled_names if is_likely_name(name.strip()))\n\n        potential_names = compiled_patterns['alt_name'].findall(text_content)\n        names.update(name.strip() for name in potential_names if is_likely_name(name.strip()))\n\n        # Extract addresses using primary pattern\n        addresses = list(set(compiled_patterns['address'].findall(text_content)))\n\n        # Filter out irrelevant addresses\n        addresses = [addr.strip() for addr in addresses if len(addr.strip().split()) > 2]\n\n        # Extract all href attributes\n        href_pattern = re.compile(r'href=[\\'\"]?([^\\'\" >]+)', re.IGNORECASE)\n        hrefs = href_pattern.findall(html)\n\n        # Extract social media links\n        social_links = {}\n        for platform, pattern in compiled_patterns['social_links'].items():\n            matches = [href for href in hrefs if pattern.search(href)]\n            if matches:\n                social_links[platform] = list(set(matches))\n\n        return {\n            'emails': emails,\n            'phones': phones,\n            'fax': fax_numbers,\n            'names': list(names),\n            'addresses': addresses,\n            'social_links': social_links\n        }\n\n    except Exception as e:\n        print(f\"Error in extraction: {str(e)}\")\n        return {\n            'emails': [],\n            'phones': [],\n            'fax': [],\n            'names': [],\n            'addresses': [],\n            'social_links': {}\n        }\n\n# Get HTML content from input_data provided by Zapier\nhtml_content = input_data.get('html', '')\n\n# Process HTML and prepare output\ntry:\n    if not html_content:\n        output = {\n            'success': False,\n            'error': 'No HTML content provided',\n            'data': None\n        }\n    else:\n        result = extract_info(html_content)\n        output = {\n            'success': True,\n            'error': None,\n            'data': result\n        }\n\nexcept Exception as e:\n    output = {\n        'success': False,\n        'error': str(e),\n        'data': None\n    }\n\n# Print the final output with required ID\nprint(json.dumps({\n    'output': output,\n    'id': '5fgunBIH5nfcaPgwiymwcjuT28AwOIJs'\n}))", "input": {"html": "{{269094725__html}}"}}, "meta": {"$editor": {"has_automatic_issues": false, "$zap_guesser": {"generated_by_ai": true}}, "parammap": {}, "stepTitle": "Parse the HTML"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269094725, "root_id": 269094723, "action": "01929fad-d3f6-c57c-1b40-ef8691be1f86", "selected_api": "CodeCLIAPI@1.0.1", "title": "Parse the HTML", "authentication_id": null, "created_at": "2024-11-27T09:14:16+00:00", "last_changed": "2024-11-27T09:46:45+00:00"}, "269094727": {"id": 269094727, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"storage_source": "one_drive", "folder_id": "01HP6PBRQ3HBO6QF7FOVCKMDW3YKCHI4FK", "workbook_id": "01HP6PBRSPIHYYUPNLJ5GZOKMV4PMDONQE", "worksheet_id": "{7588E092-B2B8-4C50-8F36-00177F236D36}", "COL$A": ["{{269094726__data__emails}}"], "COL$B": ["{{269094726__data__phones}}"], "COL$C": ["{{269094726__data__fax}}"], "COL$D": ["{{269094726__data__names}}"], "COL$E": ["{{269094726__data__addresses}}"], "COL$F": ["{{269094726__data__social_links__facebook}}"]}, "meta": {"$editor": {"has_automatic_issues": false, "$zap_guesser": {"$ai_field_suggestions": {"fields": {"COL$A": ["268045568__body_plain"], "COL$B": ["268045571__data__phones"], "COL$C": ["268045571__data__fax"], "COL$D": ["268045571__data__names"], "COL$E": ["268045568__body_plain", "268045571__data__addresses"], "COL$F": ["268045571__data__social_links__facebook", "268045571__data__social_links__facebook"]}}}}, "parammap": {"folder_id": "Zapier Automation", "workbook_id": "Auto AI Mapping Pre-Clinical.xlsx", "worksheet_id": "Raw Data"}}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269094726, "root_id": 269094723, "action": "add_rows", "selected_api": "ExcelCLIAPI@1.4.2", "title": null, "authentication_id": 50800439, "created_at": "2024-11-27T09:14:16+00:00", "last_changed": "2024-11-27T09:46:45+00:00"}, "269094728": {"id": 269094728, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"model": "gpt-3.5-turbo-instruct", "temperature": "0.7", "top_p": "1", "frequency_penalty": "0", "presence_penalty": "0", "prompt": "I have this parsed data from a website - \n{{269094726__data__names}}\n{{269094726__data__addresses}}\n{{269094726__data__social_links__instagram}}\n{{269094726__data__social_links__facebook}}\nI want you to make sense of this information - look up the information online, and have a write-up about the institution this information is related to. \nSee if you can also find names of people associated with this institution. \nThe output should be text. "}, "meta": {"$editor": {"has_automatic_issues": true, "$zap_guesser": {"$ai_field_suggestions": {"choices": {"model": ["gpt-3.5-turbo-instruct"]}}}}, "parammap": {"model": "gpt-3.5-turbo-instruct"}, "stepTitle": "Find General Information"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269094727, "root_id": 269094723, "action": "send_prompt", "selected_api": "OpenAiCLIAPI@1.19.2", "title": "Find General Information", "authentication_id": 50832433, "created_at": "2024-11-27T09:14:16+00:00", "last_changed": "2024-11-27T09:46:45+00:00"}, "269094729": {"id": 269094729, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"model": "gpt-3.5-turbo-instruct", "temperature": "0.7", "top_p": "1", "frequency_penalty": "0", "presence_penalty": "0", "prompt": "I have this list of parsed information from a website - \n{{269094726__data__addresses}}\n\n{{269094726__data__social_links__facebook}}\n{{269094726__data__social_links__instagram}}\n{{269094726__data__names}}\nI want you to make sense of the information - use online searches. \nI want you to find the address and validate it using Google searches, or by navigating to the links above. \nThen output just the validated address. \nThere can be no other answers than the validated address. Use the internet. \nThe output should be just the address of the business in that locality's format"}, "meta": {"$editor": {"has_automatic_issues": true}, "parammap": {"model": "gpt-3.5-turbo-instruct"}, "stepTitle": "Address Validation by AI"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269094728, "root_id": 269094723, "action": "send_prompt", "selected_api": "OpenAiCLIAPI@1.19.2", "title": "Address Validation by AI", "authentication_id": 50832433, "created_at": "2024-11-27T09:14:16+00:00", "last_changed": "2024-11-27T09:46:45+00:00"}, "269094730": {"id": 269094730, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"model": "gpt-3.5-turbo-instruct", "temperature": "0.7", "top_p": "1", "frequency_penalty": "0", "presence_penalty": "0", "prompt": "This is an extract from a parsed HTML (provided information) - it could include links, names, keywords, phone numbers, emails, social media accounts, etc. \n{{269094723__body_plain}}\n{{269094726__data__social_links__instagram}}\n{{269094726__data__social_links__facebook}}\n{{269094726__data__phones}}\n{{269094726__data__fax}}\n{{269094726__data__emails}}\n{{269094726__data__names}}\n{{269094726__data__addresses}}\n\nYour task is to extract, organize, and validate information using the internet based on the following categories:\n\t1.\tCompany name (EN)\n\t2.\tCompany name (Local Language if any)\n\t3.\tCompany Website\n\t4.\tCompany owner\n\t5.\tVET/CRO/BIO/MED/Academia\n\t6.\tCountry\n\t7.\tLF MRI / HF MRI / CT SCAN\n\t8.\tMRI Manufacturer\n\t9.\tMRI Type\n\t10.\tMRI name\n\t11.\tCivility\n\t12.\tContact first name\n\t13.\tContact last name\n\t14.\tPosition\n\t15.\tEmail\n\t16.\tPhone\n\t17.\tComments\n\t18.\tStatus\n\nInstructions:\n\t1.\tuse the provided information to extract the required details.\n\t2.\tOrganize the extracted data into a JSON object where each category is a key and the corresponding extracted information is its value.\n\t3.\tIf a category is not found, use Google searches, whatever resource you want to find the correct information, then if all else fails return null for that category.\n\t4.\tFormat the output as JSON, ensuring it is easy to load and process using Python.\n        5. if there are multiple values in one category, separate the values by a comma ','\n\nOutput Example:\n\n{\n  \"Company name (EN)\": \"ABC Corp\",\n  \"Company name (Local Language if any)\": \"\u682a\u5f0f\u4f1a\u793eABC\",\n  \"Company Website\": \"https://www.abccorp.com\",\n  \"Company owner\": \"John Doe\",\n  \"VET/CRO/BIO/MED/Academia\": \"VET\",\n  \"Country\": \"Japan\",\n  \"LF MRI / HF MRI / CT SCAN\": \"HF MRI\",\n  \"MRI Manufacturer\": \"GE Healthcare\",\n  \"MRI Type\": \"High-field\",\n  \"MRI name\": \"Signa HDx\",\n  \"Civility\": \"Mr.\",\n  \"Contact first name\": \"John\",\n  \"Contact last name\": \"Smith\",\n  \"Position\": \"CEO\",\n  \"Email\": \"john.smith@abccorp.com\",\n  \"Phone\": \"+81 3-1234-5678\",\n  \"Comments\": \"Key stakeholder in HF MRI solutions.\",\n  \"Status\": \"Active\"\n}\n\nNote: Ensure to accurately match HTML elements to these categories, relying on tags, classes, or attributes within the provided HTML. Return an organized JSON object even if not all information is present.\n\nOutput format: JSON as shown above."}, "meta": {"$editor": {"has_automatic_issues": true, "$zap_guesser": {"$ai_field_suggestions": {"choices": {"model": ["gpt-4o-2024-11-20"]}, "fields": {"prompt": ["268045568__body_plain"]}}}}, "parammap": {"model": "gpt-3.5-turbo-instruct"}, "stepTitle": "Total Info Grab"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269094729, "root_id": 269094723, "action": "send_prompt", "selected_api": "OpenAiCLIAPI@1.19.2", "title": "Total Info Grab", "authentication_id": 50832433, "created_at": "2024-11-27T09:14:16+00:00", "last_changed": "2024-11-27T09:46:45+00:00"}, "269094731": {"id": 269094731, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"code": "\nimport json\n\n# Get the input JSON string, defaulting to an empty JSON object if not found\nai_input = input_data.get('AI_input', '{}')\n\nif not ai_input.strip():\n    # Handle empty or whitespace-only input gracefully\n    ai_input = '{}'\n\ntry:\n    # Parse the JSON string into a Python dictionary\n    data = json.loads(ai_input)\nexcept json.JSONDecodeError:\n    # Handle invalid JSON gracefully\n    data = {}  # Default to an empty dictionary\n\n# Define the output dictionary, handling missing keys with default empty strings\noutput = {\n    \"company_name_en\": data.get(\"Company name (EN)\", \"\"),\n    \"company_name_local\": data.get(\"Company name (Local Language if any)\", \"\"),\n    \"company_website\": data.get(\"Company Website\", \"\"),\n    \"company_owner\": data.get(\"Company owner\", \"\"),\n    \"vet_cro_bio_med_academia\": data.get(\"VET/CRO/BIO/MED/Academia\", \"\"),\n    \"country\": data.get(\"Country\", \"\"),\n    \"lf_mri_hf_mri_ct_scan\": data.get(\"LF MRI / HF MRI / CT SCAN\", \"\"),\n    \"mri_manufacturer\": data.get(\"MRI Manufacturer\", \"\"),\n    \"mri_type\": data.get(\"MRI Type\", \"\"),\n    \"mri_name\": data.get(\"MRI name\", \"\"),\n    \"civility\": data.get(\"Civility\", \"\"),\n    \"contact_first_name\": data.get(\"Contact first name\", \"\"),\n    \"contact_last_name\": data.get(\"Contact last name\", \"\"),\n    \"position\": data.get(\"Position\", \"\"),\n    \"email\": data.get(\"Email\", \"\"),\n    \"phone\": data.get(\"Phone\", \"\"),\n    \"comments\": data.get(\"Comments\", \"\"),\n    \"status\": data.get(\"Status\", \"\")\n}\n", "input": {"AI_input": "{{269094730__choices[]text}}"}}, "meta": {"$editor": {"has_automatic_issues": false, "$zap_guesser": {"generated_by_ai": true}}, "parammap": {}}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269094730, "root_id": 269094723, "action": "01929fad-d3f6-c57c-1b40-ef8691be1f86", "selected_api": "CodeCLIAPI@1.0.1", "title": null, "authentication_id": null, "created_at": "2024-11-27T09:14:16+00:00", "last_changed": "2024-11-27T09:46:45+00:00"}, "269094732": {"id": 269094732, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"storage_source": "one_drive", "folder_id": "01HP6PBRQ3HBO6QF7FOVCKMDW3YKCHI4FK", "workbook_id": "01HP6PBRSPIHYYUPNLJ5GZOKMV4PMDONQE", "worksheet_id": "{367000F0-2DCD-4590-8C4A-DE1FF7E05F5C}", "COL$A": ["{{269094731__company_name_en}}, {{269094731__company_name_local}}"], "COL$B": ["{{269094731__company_website}}"], "COL$C": ["{{269094731__company_owner}}"], "COL$D": ["{{269094731__vet_cro_bio_med_academia}}"], "COL$E": ["{{269094731__country}}"], "COL$F": ["{{269094731__lf_mri_hf_mri_ct_scan}}"], "COL$G": ["{{269094731__mri_manufacturer}}"], "COL$H": ["{{269094731__mri_type}}"], "COL$I": ["{{269094731__mri_name}}"], "COL$J": ["{{269094731__civility}}"], "COL$K": ["{{269094731__contact_first_name}}"], "COL$L": ["{{269094731__contact_last_name}}"], "COL$M": ["{{269094731__position}}"], "COL$O": ["{{269094731__phone}}"], "COL$P": ["{{269094731__comments}}"], "COL$Q": ["{{269094731__status}}"], "COL$R": ["{{269094728__choices[]text}}"], "COL$S": ["{{269094729__choices[]text}}"], "COL$T": [], "COL$N": ["{{269094731__email}}"]}, "meta": {"$editor": {"has_automatic_issues": false, "$zap_guesser": {"$ai_field_suggestions": {"fields": {"COL$A": ["268764428__company_name_local"]}}}}, "parammap": {"folder_id": "Zapier Automation", "workbook_id": "Auto AI Mapping Pre-Clinical.xlsx", "worksheet_id": "Raw Data Validated by AI"}}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269094731, "root_id": 269094723, "action": "add_rows", "selected_api": "ExcelCLIAPI@1.4.2", "title": null, "authentication_id": 50800439, "created_at": "2024-11-27T09:14:16+00:00", "last_changed": "2024-11-27T09:46:45+00:00"}}}, {"id": 269094755, "title": "Clinical Working Linear Simple", "status": "off", "nodes": {"269094755": {"id": 269094755, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "read", "params": {"label_ids": "INBOX"}, "meta": {"$editor": {"$zap_guesser": {"generated_by_ai": false, "zap_guesser_execution_id": "0192e9c9-d0a7-7e64-adf1-7f3c50f4c265", "$ai_field_suggestions": {"choices": {"label_ids": ["CHAT"]}}}, "has_automatic_issues": false}, "parammap": {"label_ids": "INBOX"}, "stepTitle": "New Inbound Email"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": null, "root_id": null, "action": "message", "selected_api": "GoogleMailV2CLIAPI@1.1.11", "title": "Clinical Working Linear Simple", "authentication_id": 50864737, "created_at": "2024-11-27T09:14:24+00:00", "last_changed": "2024-11-28T10:46:30+00:00"}, "269094756": {"id": 269094756, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"storage_source": "one_drive", "folder_id": "01HP6PBRQ3HBO6QF7FOVCKMDW3YKCHI4FK", "workbook_id": "01HP6PBRSOSJJOPPIX3RGJYBJDEQXMNIJZ", "worksheet_id": "{00000000-0001-0000-0000-000000000000}", "COL$A": "{{269094755__body_plain}}"}, "meta": {"$editor": {"has_automatic_issues": false, "$zap_guesser": {"$ai_field_suggestions": {"fields": {"COL$A": ["268045568__body_plain"], "COL$C": ["268045568__from__email", "268045568__body_plain"]}}}}, "parammap": {"folder_id": "Zapier Automation", "workbook_id": "Auto AI Mapping Clinical.xlsx", "worksheet_id": "Links Raw Data"}, "stepTitle": "Add Row"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269094755, "root_id": 269094755, "action": "add_row", "selected_api": "ExcelCLIAPI@1.4.2", "title": "Add Row", "authentication_id": 50800439, "created_at": "2024-11-27T09:14:24+00:00", "last_changed": "2024-11-27T09:58:02+00:00"}, "269094757": {"id": 269094757, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"code": "import requests\nfrom requests.adapters import HTTPAdapter\nfrom urllib.parse import urlparse, urlunparse\nfrom bs4 import BeautifulSoup\n\n# Check urllib3 version to set the correct parameter\ntry:\n    import urllib3\n    urllib3_version = urllib3.__version__\nexcept ImportError:\n    # Default to an older version if urllib3 is not available\n    urllib3_version = '1.25.0'\n\n# Define the Retry class with the appropriate argument\ndef get_retry():\n    from urllib3.util import Retry\n    if urllib3_version >= '1.26.0':\n        return Retry(\n            total=3,\n            backoff_factor=0.3,\n            status_forcelist=[500, 502, 503, 504],\n            allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\", \"POST\"]\n        )\n    else:\n        return Retry(\n            total=3,\n            backoff_factor=0.3,\n            status_forcelist=[500, 502, 503, 504],\n            method_whitelist=[\"HEAD\", \"GET\", \"OPTIONS\", \"POST\"]\n        )\n\n# Get the URL from the input data\nurl = input_data.get('url', '').strip()\n\n# Clean the URL to remove any unwanted characters like '\\r' and '\\n'\nparsed_url = urlparse(url)\nclean_path = parsed_url.path.replace('\\r', '').replace('\\n', '')\nclean_url = urlunparse(parsed_url._replace(path=clean_path))\n\nheaders = {\n    'User-Agent': (\n        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n        '(KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n    )\n}\n\n# Set up a session with retry strategy\nsession = requests.Session()\n\nretry_strategy = get_retry()\nadapter = HTTPAdapter(max_retries=retry_strategy)\nsession.mount('http://', adapter)\nsession.mount('https://', adapter)\n\ntry:\n    # Fetch the HTML content with a timeout\n    response = session.get(clean_url, headers=headers, timeout=5)\n    response.raise_for_status()  # Raise an exception for bad status codes\n\n    # Parse the HTML using BeautifulSoup\n    soup = BeautifulSoup(response.text, 'html.parser')\n\n    # Extract the entire content of the <body> tag, including all HTML\n    body_tag = soup.body\n    if body_tag:\n        body_html = str(body_tag)\n    else:\n        body_html = ''\n\n    # Output the body HTML\n    output = {'html': body_html.strip()}\n\nexcept requests.exceptions.RequestException as e:\n    output = {'error': f'Error fetching URL: {e}'}\n\n# For demonstration purposes, print the output\nprint(output)", "input": {"url": "{{269094755__body_plain}}"}}, "meta": {"$editor": {"has_automatic_issues": false, "$zap_guesser": {"generated_by_ai": true}}, "parammap": {}, "stepTitle": "Grab HTML"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269094756, "root_id": 269094755, "action": "01929fad-d3f6-c57c-1b40-ef8691be1f86", "selected_api": "CodeCLIAPI@1.0.1", "title": "Grab HTML", "authentication_id": null, "created_at": "2024-11-27T09:14:24+00:00", "last_changed": "2024-11-27T09:58:02+00:00"}, "269094758": {"id": 269094758, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"code": "import re  # Using the standard 're' module\nimport json\n\n# Enhanced regex patterns compatible with 're' module\npatterns = {\n    'email': r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\",\n\n    'phone': r\"(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{1,4}\\)?[-.\\s]?)*\\d{3,4}\",\n\n    'fax': r\"(?:Fax|F|\u4f20\u771f|\u30d5\u30a1\u30c3\u30af\u30b9|\ud329\uc2a4|\u092b\u0948\u0915\u094d\u0938|\u0442\u0435\u043b|\u0444\u0430\u043a\u0441)[.:]?\\s*(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{1,4}\\)?[-.\\s]?)*\\d{3,4}\",\n\n    'name': r\"\"\"\n        (?:(?:Dr|Mr|Mrs|Ms|Miss|Professor|Prof|MD|DVM|Docteur|V\u00e9t\u00e9rinaire|Asv|\n        \u0938\u0930\u094d\u0935|\u0936\u094d\u0930\u0940|\u0936\u094d\u0930\u0940\u092e\u0924\u0940|Herr|Frau|M\\.|Mme|Mlle|Se\u00f1or|Se\u00f1ora|Se\u00f1orita|\u5148\u751f|\u5973\u58eb|\u6559\u6388|\u535a\u58eb|\uc120\uc0dd\ub2d8|\uad50\uc218\ub2d8|\n        \u0414\u0440|\u0413-\u043d|\u0413-\u0436\u0430|\u0413\u043e\u0441\u043f\u043e\u0434\u0438\u043d|\u0413\u043e\u0441\u043f\u043e\u0436\u0430|\u041f\u0440\u043e\u0444)\\.?\\s+)?\n        (?:[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)+)\n    \"\"\",\n\n    'alt_name': r\"(?:[A-Z][a-z]+(?:\\s+[A-Z][a-z]+){1,2})\",\n\n    'address': r\"\"\"\n        \\d{1,5}\\s+[A-Za-z0-9\\s.,'-]+\n    \"\"\",\n\n    'address_fallback': r\"[A-Za-z0-9\\s.,'-]+\",\n\n    'social_links': {\n        'facebook': r\"(?:https?:\\/\\/)?(?:www\\.)?(?:facebook|fb)\\.(?:com|cn)\\/[^\\/\\s]+\\/?\",\n        'twitter': r\"(?:https?:\\/\\/)?(?:www\\.)?twitter\\.com\\/[^\\/\\s]+\\/?\",\n        'instagram': r\"(?:https?:\\/\\/)?(?:www\\.)?instagram\\.com\\/[^\\/\\s]+\\/?\",\n        'linkedin': r\"(?:https?:\\/\\/)?(?:www\\.)?linkedin\\.com\\/(?:company|in)\\/[^\\/\\s]+\\/?\",\n        'youtube': r\"(?:https?:\\/\\/)?(?:www\\.)?youtube\\.com\\/(?:channel|user)\\/[^\\/\\s]+\\/?\"\n    }\n}\n\n# Compile patterns with appropriate flags\ncompiled_patterns = {\n    'email': re.compile(patterns['email'], re.IGNORECASE),\n    'phone': re.compile(patterns['phone']),\n    'fax': re.compile(patterns['fax'], re.IGNORECASE),\n    'name': re.compile(patterns['name'], re.VERBOSE),\n    'alt_name': re.compile(patterns['alt_name']),\n    'address': re.compile(patterns['address'], re.VERBOSE),\n    'address_fallback': re.compile(patterns['address_fallback']),\n    'social_links': {\n        platform: re.compile(pattern, re.IGNORECASE)\n        for platform, pattern in patterns['social_links'].items()\n    }\n}\n\ndef clean_phone(phone):\n    \"\"\"Clean phone numbers without phonenumbers library\"\"\"\n    try:\n        # Remove all non-digit characters\n        digits = re.sub(r\"\\D\", \"\", phone)\n\n        # Return digits if length is acceptable\n        if len(digits) >= 7:\n            return digits\n        else:\n            return None  # Discard short numbers\n    except Exception:\n        return None\n\ndef is_likely_name(name):\n    \"\"\"Filter out false positives in name detection\"\"\"\n    common_words = {'Home', 'Page', 'Menu', 'Contact', 'About', 'Services', 'Privacy', 'Policy',\n                    'Terms', 'Conditions', 'Search', 'Main', 'Content', 'Navigation', 'Careers',\n                    'Emergency', 'FAQ', 'Log', 'In', 'Out', 'Find', 'More'}\n    words = name.split()\n    return (\n        len(words) >= 2 and\n        not any(word in common_words for word in words) and\n        not any(char.isdigit() for char in name) and\n        all(word[0].isupper() for word in words if word)\n    )\n\ndef extract_info(html):\n    \"\"\"Extract all contact information from HTML\"\"\"\n    try:\n        # Remove style and script contents\n        html_no_style = re.sub(r'<style[^>]*>[\\s\\S]*?</style>', '', html, flags=re.IGNORECASE)\n        html_no_script = re.sub(r'<script[^>]*>[\\s\\S]*?</script>', '', html_no_style, flags=re.IGNORECASE)\n\n        # Convert HTML to text\n        text_content = re.sub(r'<[^>]+>', ' ', html_no_script, flags=re.IGNORECASE)\n        text_content = re.sub(r'\\s+', ' ', text_content, flags=re.IGNORECASE).strip()\n\n        # Extract emails\n        emails = list(set(compiled_patterns['email'].findall(text_content)))\n\n        # Extract and clean phone numbers\n        phones = []\n        phone_matches = compiled_patterns['phone'].findall(text_content)\n        for phone in phone_matches:\n            cleaned = clean_phone(phone)\n            if cleaned and cleaned not in phones:\n                phones.append(cleaned)\n\n        # Extract and clean fax numbers\n        fax_numbers = []\n        fax_contexts = compiled_patterns['fax'].finditer(text_content)\n        for match in fax_contexts:\n            fax = clean_phone(match.group())\n            if fax and fax not in fax_numbers:\n                fax_numbers.append(fax)\n\n        # Extract names\n        names = set()\n        titled_names = compiled_patterns['name'].findall(text_content)\n        names.update(name.strip() for name in titled_names if is_likely_name(name.strip()))\n\n        potential_names = compiled_patterns['alt_name'].findall(text_content)\n        names.update(name.strip() for name in potential_names if is_likely_name(name.strip()))\n\n        # Extract addresses using primary pattern\n        addresses = list(set(compiled_patterns['address'].findall(text_content)))\n\n        # Filter out irrelevant addresses\n        addresses = [addr.strip() for addr in addresses if len(addr.strip().split()) > 2]\n\n        # Extract all href attributes\n        href_pattern = re.compile(r'href=[\\'\"]?([^\\'\" >]+)', re.IGNORECASE)\n        hrefs = href_pattern.findall(html)\n\n        # Extract social media links\n        social_links = {}\n        for platform, pattern in compiled_patterns['social_links'].items():\n            matches = [href for href in hrefs if pattern.search(href)]\n            if matches:\n                social_links[platform] = list(set(matches))\n\n        return {\n            'emails': emails,\n            'phones': phones,\n            'fax': fax_numbers,\n            'names': list(names),\n            'addresses': addresses,\n            'social_links': social_links\n        }\n\n    except Exception as e:\n        print(f\"Error in extraction: {str(e)}\")\n        return {\n            'emails': [],\n            'phones': [],\n            'fax': [],\n            'names': [],\n            'addresses': [],\n            'social_links': {}\n        }\n\n# Get HTML content from input_data provided by Zapier\nhtml_content = input_data.get('html', '')\n\n# Process HTML and prepare output\ntry:\n    if not html_content:\n        output = {\n            'success': False,\n            'error': 'No HTML content provided',\n            'data': None\n        }\n    else:\n        result = extract_info(html_content)\n        output = {\n            'success': True,\n            'error': None,\n            'data': result\n        }\n\nexcept Exception as e:\n    output = {\n        'success': False,\n        'error': str(e),\n        'data': None\n    }\n\n# Print the final output with required ID\nprint(json.dumps({\n    'output': output,\n    'id': '5fgunBIH5nfcaPgwiymwcjuT28AwOIJs'\n}))", "input": {"html": "{{269094757__html}}"}}, "meta": {"$editor": {"has_automatic_issues": false, "$zap_guesser": {"generated_by_ai": true}}, "parammap": {}, "stepTitle": "Parse the HTML"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269094757, "root_id": 269094755, "action": "01929fad-d3f6-c57c-1b40-ef8691be1f86", "selected_api": "CodeCLIAPI@1.0.1", "title": "Parse the HTML", "authentication_id": null, "created_at": "2024-11-27T09:14:24+00:00", "last_changed": "2024-11-27T09:58:02+00:00"}, "269094759": {"id": 269094759, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"storage_source": "one_drive", "folder_id": "01HP6PBRQ3HBO6QF7FOVCKMDW3YKCHI4FK", "workbook_id": "01HP6PBRSOSJJOPPIX3RGJYBJDEQXMNIJZ", "worksheet_id": "{4642567E-A856-451D-BA04-DF0EB96A16B8}", "COL$A": ["{{269094758__data__emails}}"], "COL$B": ["{{269094758__data__phones}}"], "COL$C": ["{{269094758__data__fax}}"], "COL$D": ["{{269094758__data__names}}"], "COL$E": ["{{269094758__data__addresses}}"], "COL$F": ["{{269094758__data__social_links__facebook}}"]}, "meta": {"$editor": {"has_automatic_issues": false, "$zap_guesser": {"$ai_field_suggestions": {"fields": {"COL$A": ["268045568__body_plain"], "COL$B": ["268045571__data__phones"], "COL$C": ["268045571__data__fax"], "COL$D": ["268045571__data__names"], "COL$E": ["268045568__body_plain", "268045571__data__addresses"], "COL$F": ["268045571__data__social_links__facebook", "268045571__data__social_links__facebook"]}}}}, "parammap": {"folder_id": "Zapier Automation", "workbook_id": "Auto AI Mapping Clinical.xlsx", "worksheet_id": "Raw Data"}}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269094758, "root_id": 269094755, "action": "add_rows", "selected_api": "ExcelCLIAPI@1.4.2", "title": null, "authentication_id": 50800439, "created_at": "2024-11-27T09:14:24+00:00", "last_changed": "2024-11-27T09:58:02+00:00"}, "269094760": {"id": 269094760, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"model": "gpt-3.5-turbo-instruct", "temperature": "0.7", "top_p": "1", "frequency_penalty": "0", "presence_penalty": "0", "prompt": "I have this parsed data from a website - \n{{269094758__data__names}}\n{{269094758__data__addresses}}\n{{269094758__data__social_links__instagram}}\n{{269094758__data__social_links__facebook}}\nI want you to make sense of this information - look up the information online, and have a write-up about the institution this information is related to. \nSee if you can also find names of people associated with this institution. \nThe output should be text. "}, "meta": {"$editor": {"has_automatic_issues": true, "$zap_guesser": {"$ai_field_suggestions": {"choices": {"model": ["gpt-3.5-turbo-instruct"]}}}}, "parammap": {"model": "gpt-3.5-turbo-instruct"}, "stepTitle": "Find General Information"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269094759, "root_id": 269094755, "action": "send_prompt", "selected_api": "OpenAiCLIAPI@1.19.2", "title": "Find General Information", "authentication_id": 50832433, "created_at": "2024-11-27T09:14:24+00:00", "last_changed": "2024-11-27T09:58:02+00:00"}, "269094761": {"id": 269094761, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"model": "gpt-3.5-turbo-instruct", "temperature": "0.7", "top_p": "1", "frequency_penalty": "0", "presence_penalty": "0", "prompt": "I have this list of parsed information from a website - \n{{269094758__data__addresses}}\n\n{{269094758__data__social_links__facebook}}\n{{269094758__data__social_links__instagram}}\n{{269094758__data__names}}\nI want you to make sense of the information - use online searches. \nI want you to find the address and validate it using Google searches, or by navigating to the links above. \nThen output just the validated address. \nThere can be no other answers than the validated address. Use the internet. \nThe output should be just the address of the business in that locality's format"}, "meta": {"$editor": {"has_automatic_issues": true}, "parammap": {"model": "gpt-3.5-turbo-instruct"}, "stepTitle": "Address Validation by AI"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269094760, "root_id": 269094755, "action": "send_prompt", "selected_api": "OpenAiCLIAPI@1.19.2", "title": "Address Validation by AI", "authentication_id": 50832433, "created_at": "2024-11-27T09:14:24+00:00", "last_changed": "2024-11-27T09:58:02+00:00"}, "269094762": {"id": 269094762, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"model": "gpt-3.5-turbo-instruct", "temperature": "0.7", "top_p": "1", "frequency_penalty": "0", "presence_penalty": "0", "prompt": "This is an extract from a parsed HTML (provided information) - it could include links, names, keywords, phone numbers, emails, social media accounts, etc. \n{{269094755__body_plain}}\n{{269094758__data__social_links__instagram}}\n{{269094758__data__social_links__facebook}}\n{{269094758__data__phones}}\n{{269094758__data__fax}}\n{{269094758__data__emails}}\n{{269094758__data__names}}\n{{269094758__data__addresses}}\n\nYour task is to extract, organize, and validate information using the internet based on the following categories:\n\t1.\tCompany name (EN)\n\t2.\tCompany name (Local Language if any)\n\t3.\tCompany Website\n\t4.\tCompany owner\n\t5.\tVET/CRO/BIO/MED/Academia\n\t6.\tCountry\n\t7.\tLF MRI / HF MRI / CT SCAN\n\t8.\tMRI Manufacturer\n\t9.\tMRI Type\n\t10.\tMRI name\n\t11.\tCivility\n\t12.\tContact first name\n\t13.\tContact last name\n\t14.\tPosition\n\t15.\tEmail\n\t16.\tPhone\n\t17.\tComments\n\t18.\tStatus\n\nInstructions:\n\t1.\tuse the provided information to extract the required details.\n\t2.\tOrganize the extracted data into a JSON object where each category is a key and the corresponding extracted information is its value.\n\t3.\tIf a category is not found, use Google searches, whatever resource you want to find the correct information, then if all else fails return null for that category.\n\t4.\tFormat the output as JSON, ensuring it is easy to load and process using Python.\n        5. if there are multiple values in one category, separate the values by a comma ','\n\nOutput Example:\n\n{\n  \"Company name (EN)\": \"ABC Corp\",\n  \"Company name (Local Language if any)\": \"\u682a\u5f0f\u4f1a\u793eABC\",\n  \"Company Website\": \"https://www.abccorp.com\",\n  \"Company owner\": \"John Doe\",\n  \"VET/CRO/BIO/MED/Academia\": \"VET\",\n  \"Country\": \"Japan\",\n  \"LF MRI / HF MRI / CT SCAN\": \"HF MRI\",\n  \"MRI Manufacturer\": \"GE Healthcare\",\n  \"MRI Type\": \"High-field\",\n  \"MRI name\": \"Signa HDx\",\n  \"Civility\": \"Mr.\",\n  \"Contact first name\": \"John\",\n  \"Contact last name\": \"Smith\",\n  \"Position\": \"CEO\",\n  \"Email\": \"john.smith@abccorp.com\",\n  \"Phone\": \"+81 3-1234-5678\",\n  \"Comments\": \"Key stakeholder in HF MRI solutions.\",\n  \"Status\": \"Active\"\n}\n\nNote: Ensure to accurately match HTML elements to these categories, relying on tags, classes, or attributes within the provided HTML. Return an organized JSON object even if not all information is present.\n\nOutput format: JSON as shown above."}, "meta": {"$editor": {"has_automatic_issues": true, "$zap_guesser": {"$ai_field_suggestions": {"choices": {"model": ["gpt-4o-2024-11-20"]}, "fields": {"prompt": ["268045568__body_plain"]}}}}, "parammap": {"model": "gpt-3.5-turbo-instruct"}, "stepTitle": "Total Info Grab"}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269094761, "root_id": 269094755, "action": "send_prompt", "selected_api": "OpenAiCLIAPI@1.19.2", "title": "Total Info Grab", "authentication_id": 50832433, "created_at": "2024-11-27T09:14:24+00:00", "last_changed": "2024-11-27T09:58:02+00:00"}, "269094763": {"id": 269094763, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"code": "\nimport json\n\n# Get the input JSON string, defaulting to an empty JSON object if not found\nai_input = input_data.get('AI_input', '{}')\n\nif not ai_input.strip():\n    # Handle empty or whitespace-only input gracefully\n    ai_input = '{}'\n\ntry:\n    # Parse the JSON string into a Python dictionary\n    data = json.loads(ai_input)\nexcept json.JSONDecodeError:\n    # Handle invalid JSON gracefully\n    data = {}  # Default to an empty dictionary\n\n# Define the output dictionary, handling missing keys with default empty strings\noutput = {\n    \"company_name_en\": data.get(\"Company name (EN)\", \"\"),\n    \"company_name_local\": data.get(\"Company name (Local Language if any)\", \"\"),\n    \"company_website\": data.get(\"Company Website\", \"\"),\n    \"company_owner\": data.get(\"Company owner\", \"\"),\n    \"vet_cro_bio_med_academia\": data.get(\"VET/CRO/BIO/MED/Academia\", \"\"),\n    \"country\": data.get(\"Country\", \"\"),\n    \"lf_mri_hf_mri_ct_scan\": data.get(\"LF MRI / HF MRI / CT SCAN\", \"\"),\n    \"mri_manufacturer\": data.get(\"MRI Manufacturer\", \"\"),\n    \"mri_type\": data.get(\"MRI Type\", \"\"),\n    \"mri_name\": data.get(\"MRI name\", \"\"),\n    \"civility\": data.get(\"Civility\", \"\"),\n    \"contact_first_name\": data.get(\"Contact first name\", \"\"),\n    \"contact_last_name\": data.get(\"Contact last name\", \"\"),\n    \"position\": data.get(\"Position\", \"\"),\n    \"email\": data.get(\"Email\", \"\"),\n    \"phone\": data.get(\"Phone\", \"\"),\n    \"comments\": data.get(\"Comments\", \"\"),\n    \"status\": data.get(\"Status\", \"\")\n}\n", "input": {"AI_input": "{{269094762__choices[]text}}"}}, "meta": {"$editor": {"has_automatic_issues": false, "$zap_guesser": {"generated_by_ai": true}}, "parammap": {}}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269094762, "root_id": 269094755, "action": "01929fad-d3f6-c57c-1b40-ef8691be1f86", "selected_api": "CodeCLIAPI@1.0.1", "title": null, "authentication_id": null, "created_at": "2024-11-27T09:14:24+00:00", "last_changed": "2024-11-27T09:58:02+00:00"}, "269094764": {"id": 269094764, "account_id": 20509841, "customuser_id": 20522317, "paused": true, "type_of": "write", "params": {"storage_source": "one_drive", "folder_id": "01HP6PBRQ3HBO6QF7FOVCKMDW3YKCHI4FK", "workbook_id": "01HP6PBRSOSJJOPPIX3RGJYBJDEQXMNIJZ", "worksheet_id": "{E19A917D-823C-4833-A88A-3062305F8BF7}", "COL$A": ["{{269094763__company_name_en}}, {{269094763__company_name_local}}"], "COL$B": ["{{269094763__company_website}}"], "COL$C": ["{{269094763__company_owner}}"], "COL$D": ["{{269094763__vet_cro_bio_med_academia}}"], "COL$E": ["{{269094763__country}}"], "COL$F": ["{{269094763__lf_mri_hf_mri_ct_scan}}"], "COL$G": ["{{269094763__mri_manufacturer}}"], "COL$H": ["{{269094763__mri_type}}"], "COL$I": ["{{269094763__mri_name}}"], "COL$J": ["{{269094763__civility}}"], "COL$K": ["{{269094763__contact_first_name}}"], "COL$L": ["{{269094763__contact_last_name}}"], "COL$M": ["{{269094763__position}}"], "COL$O": ["{{269094763__phone}}"], "COL$P": ["{{269094763__comments}}"], "COL$Q": ["{{269094763__status}}"], "COL$R": ["{{269094760__choices[]text}}"], "COL$S": ["{{269094761__choices[]text}}"], "COL$T": [], "COL$N": ["{{269094763__email}}"]}, "meta": {"$editor": {"has_automatic_issues": false, "$zap_guesser": {"$ai_field_suggestions": {"fields": {"COL$A": ["268764428__company_name_local"]}}}}, "parammap": {"folder_id": "Zapier Automation", "workbook_id": "Auto AI Mapping Clinical.xlsx", "worksheet_id": "Raw Data Validated by AI"}}, "triple_stores": {"copied_from": null, "created_by": null, "polling_interval_override": 0, "block_and_release_limit_override": 0, "spread_tasks": 1}, "folders": null, "parent_id": 269094763, "root_id": 269094755, "action": "add_rows", "selected_api": "ExcelCLIAPI@1.4.2", "title": null, "authentication_id": 50800439, "created_at": "2024-11-27T09:14:24+00:00", "last_changed": "2024-11-27T09:58:02+00:00"}}}]}